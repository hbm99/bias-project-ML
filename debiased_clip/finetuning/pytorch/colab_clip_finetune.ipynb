{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"respective-athletics"},"source":["# Installing CLIP"]},{"cell_type":"code","execution_count":53,"id":"b32a5554","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6115,"status":"ok","timestamp":1685725788831,"user":{"displayName":"Hansel Blanco","userId":"12089062727487601990"},"user_tz":240},"id":"OQCNUADDVlaE","outputId":"a44d22e4-d424-48c9-d97d-4d35447dd898"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"id":"ff578b10","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QAeKujTrVlI4","outputId":"cb8ee962-f423-49df-8022-977ed530e436"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  gdrive/MyDrive/utkface.zip\n","replace __MACOSX/._utkface? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}],"source":["!unzip gdrive/MyDrive/utkface.zip"]},{"cell_type":"code","execution_count":null,"id":"de702b9c","metadata":{"colab":{"background_save":true},"id":"GsgH7xVpWcgt"},"outputs":[],"source":["import os\n","import random as rd\n","\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold\n","\n","RACE_MAPPER = {0:'white', 1:'black',2: 'asian',3: 'indian', 4:'other'}\n","GENDER_MAPPER = {0:'male',1:'female'}\n","\n","\n","def data_selection(ds_path: str = 'utkface/', k: int = 5):\n","  \n","  df = load_dataset(ds_path)\n","  \n","  print(df)\n","\n","  df = map_values(df)\n","\n","  print(df)\n","  \n","  # Stratified KFold\n","  skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=1)\n","\n","  X = df['filepath']\n","  y = df['gender'] + df['race']\n","  r = rd.randint(0, k - 1)\n","  train_idx = []\n","  test_idx = []\n","\n","  for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n","      if i == r:\n","          train_idx=train_index\n","          test_idx=test_index\n","          break\n","\n","  train_data = df.iloc[train_idx].reset_index(drop=True)\n","  test_data = df.iloc[test_idx].reset_index(drop=True)\n","\n","  print(train_data)\n","  print(test_data)\n","\n","  return train_data, test_data\n","\n","\n","def load_dataset(ds_path: str):\n","  # Loading filenames\n","  filenames = os.listdir(ds_path)\n","  \n","  try:\n","      filenames.remove('.DS_Store')\n","  except:\n","      pass\n","  \n","  # Building the dataframe\n","  df = pd.DataFrame(filenames, columns = ['filename'] )\n","  df['filepath'] = df.filename.apply(lambda x: ds_path + x )\n","  df['age'] = df.filename.apply(lambda x: int(x.split('_')[0]))\n","  df['gender'] = df.filename.apply(lambda x: int(x.split('_')[1]))\n","  df['race'] = df.filename.apply(lambda x: int(x.split('_')[-2]))\n","  \n","  return df\n","\n","def map_values(df: pd.DataFrame):\n","  for i in range(len(df)):\n","      df['gender'][i]= GENDER_MAPPER[df['gender'][i]]\n","      df['race'][i]= RACE_MAPPER[df['race'][i]]\n","  return df\n","    \n","\n"]},{"cell_type":"code","execution_count":null,"id":"5e7fe377","metadata":{"id":"concerned-porcelain"},"outputs":[],"source":["%pip install ftfy regex tqdm\n","%pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":null,"id":"e8e0296a","metadata":{"id":"packed-release"},"outputs":[],"source":["from PIL import Image\n","import torch\n","from torch import nn, optim\n","import glob\n","import os\n","import pandas as pd\n","import json\n","import numpy as np\n","import clip\n","from torch.utils.data import Dataset, DataLoader, BatchSampler\n","from sklearn.model_selection import train_test_split\n","from tqdm.notebook import tqdm\n","import random\n","from matplotlib.pyplot import imshow\n","import nltk, re, string, collections\n","from nltk.util import ngrams\n","import collections\n","from itertools import combinations\n","\n","%matplotlib inline\n","\n","BATCH_SIZE = 4\n","EPOCH = 30\n","EQ_ODDS_THRESHOLD = 0.15"]},{"attachments":{},"cell_type":"markdown","id":"f2b2f757","metadata":{"id":"complete-shipping"},"source":["# Preparing Model and Data"]},{"cell_type":"code","execution_count":null,"id":"3c3bcda7","metadata":{"id":"distributed-telling"},"outputs":[],"source":["train_df, test_df = data_selection()\n","test_df.to_json(r'test_data_df.json')\n"]},{"attachments":{},"cell_type":"markdown","id":"4873660e","metadata":{"id":"union-tyler"},"source":["## Splitting 20% for Validation"]},{"attachments":{},"cell_type":"markdown","id":"d151233b","metadata":{"id":"healthy-reporter"},"source":["## Loading Pre-trained CLIP Model and Preprocessor"]},{"cell_type":"code","execution_count":null,"id":"7f862bb0","metadata":{"id":"bee6b619"},"outputs":[],"source":["train_df_temp = train_df.sample(frac=0.8)\n","validation_df = train_df.drop(train_df_temp.index).reset_index(drop=True)\n","train_df = train_df_temp.reset_index(drop=True)\n","\n","print(len(train_df))\n","train_df_males = train_df.loc[train_df['gender'] == 'male'].sample(1000)\n","train_df_females = train_df.loc[train_df['gender'] == 'female'].sample(1000)\n","train_df = pd.concat(train_df_males, train_df_females)\n","\n","valid_df_males = validation_df.loc[validation_df['gender'] == 'male'].sample(200)\n","valid_df_females = validation_df.loc[validation_df['gender'] == 'female'].sample(200)\n","validation_df = pd.concat(valid_df_males, valid_df_females)\n","\n","len(train_df), len(validation_df), len(test_df)"]},{"cell_type":"code","execution_count":null,"id":"a384f1b6","metadata":{"id":"jewish-indianapolis"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/16\", device=device, jit=False)"]},{"attachments":{},"cell_type":"markdown","id":"406e7d47","metadata":{"id":"welcome-tolerance"},"source":["## UTKFaceDataset"]},{"cell_type":"code","execution_count":null,"id":"9adb6395","metadata":{"id":"worth-enough"},"outputs":[],"source":["class UTKFaceDataset(Dataset):\n","    def __init__(self, dataframe, preprocess):\n","        self.preprocess = preprocess\n","        self.filepath = dataframe[\"filepath\"].tolist()\n","        self.filename = dataframe[\"filename\"].tolist()\n","        self.gender = dataframe[\"gender\"].tolist()\n","        self.race = dataframe[\"race\"].tolist()\n","        self.age = dataframe[\"age\"].tolist()\n","        self.preprocessed_cache = {}\n","        for path in self.filepath:\n","            self.preprocessed_cache[path] = self.preprocess(Image.open(path))\n","\n","    def __len__(self):\n","        return len(self.filepath)\n","\n","    def __getitem__(self, idx):\n","        filepath = self.filepath[idx]\n","        filename = self.filename[idx]\n","        gender = self.gender[idx]\n","        race = self.race[idx]\n","        age = self.age[idx]\n","        image = self.preprocessed_cache[filepath]\n","        return filepath, filename, gender, race, age, image\n","\n","train_dataset = UTKFaceDataset(train_df, preprocess)\n","validation_dataset = UTKFaceDataset(validation_df, preprocess)\n","len(train_dataset), len(validation_dataset), train_dataset[0]"]},{"attachments":{},"cell_type":"markdown","id":"1829ea85","metadata":{"id":"emotional-transmission"},"source":["## BatchSampler"]},{"cell_type":"code","execution_count":null,"id":"80a0131b","metadata":{"id":"innovative-photography"},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n","validation_dataloader = DataLoader(validation_dataset, batch_size = BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":null,"id":"d6876931","metadata":{"id":"radio-trademark"},"outputs":[],"source":["for batch in train_dataloader:\n","    print(batch[2])\n","    break"]},{"attachments":{},"cell_type":"markdown","id":"5c510b3b","metadata":{"id":"appointed-passage"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"id":"e3f4eaea","metadata":{"id":"329d843f"},"outputs":[],"source":["attributes_queries = {}\n","\n","race_labels = ['black', 'white', 'asian', 'indian', 'other']\n","for label in race_labels:\n","    attributes_queries[label] = 'A photo of a person of ' + label + ' race.'\n","\n","gender_labels = ['male', 'female']\n","for label in gender_labels:\n","    attributes_queries[label] = 'A photo of a person of ' + label + ' gender.'\n","\n","print(attributes_queries)\n","\n","gender_texts = [attributes_queries[lbl] for lbl in gender_labels]\n","gender_texts = clip.tokenize(gender_texts).to(device)"]},{"cell_type":"code","execution_count":null,"id":"85433a71","metadata":{"id":"given-apache"},"outputs":[],"source":["#https://github.com/openai/CLIP/issues/57\n","def convert_models_to_fp32(model): \n","    for p in model.parameters(): \n","        p.data = p.data.float() \n","        p.grad.data = p.grad.data.float() \n","\n","if device == \"cpu\":\n","    model.float()\n","\n","loss_img = nn.CrossEntropyLoss()\n","loss_txt = nn.CrossEntropyLoss()\n","# optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n","optimizer = optim.Adam(model.parameters(), lr=1e-5)\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader)*EPOCH)"]},{"cell_type":"code","execution_count":null,"id":"d9596fe8","metadata":{"id":"2eW1SgMHT8oc"},"outputs":[],"source":["def update_eq_odds_rates(rates, labels, logits, real_values):\n","    for i in range(len(real_values)):\n","        true_value = real_values[i]\n","        pred_idx = torch.argmax(logits[i])\n","        pred_value = labels[pred_idx]\n","        \n","        if true_value == labels[pred_idx]:\n","            rates[true_value]['tp'] += 1\n","        else:\n","            rates[true_value]['fn'] += 1\n","            rates[pred_value]['fp'] += 1"]},{"cell_type":"code","execution_count":null,"id":"57e742c4","metadata":{"id":"imposed-billion"},"outputs":[],"source":["best_te_loss = 1e5\n","best_ep = -1\n","\n","# Equalized odds vars\n","best_te_bias = -1\n","tpr = lambda tp, fn: tp/ (tp + fn)\n","fpr = lambda fp, fn: fp/ (fp + fn)\n","\n","for epoch in range(EPOCH):\n","    print(f\"running epoch {epoch}, best test loss {best_te_loss} after epoch {best_ep}\")\n","    step = 0\n","    tr_loss = 0\n","    model.train()\n","    pbar = tqdm(train_dataloader, leave=False)\n","    for batch in pbar:\n","        step += 1\n","        optimizer.zero_grad()\n","\n","        images = batch[-1]\n","        \n","        images = images.to(device)\n","        logits_per_image, _ = model(images, gender_texts)\n","        # print(logits_per_image.shape)\n","        # print(logits_per_image)\n","         # torch.arange(BATCH_SIZE).to(device)\n","        ground_truth = torch.zeros((BATCH_SIZE, len(gender_labels))).to(device) # torch.arange(BATCH_SIZE).to(device)\n","        \n","        for i in range(BATCH_SIZE):\n","            truth_idx = gender_labels.index(batch[2][i])\n","            ground_truth[i, truth_idx] = 1\n","\n","        total_loss = loss_img(logits_per_image, ground_truth)\n","        total_loss.backward()\n","        tr_loss += total_loss.item()\n","        if device == \"cpu\":\n","            optimizer.step()\n","            scheduler.step()\n","        else:\n","            convert_models_to_fp32(model)\n","            optimizer.step()\n","            scheduler.step()\n","            clip.model.convert_weights(model)\n","        pbar.set_description(f\"train batchCE: {total_loss.item()}\", refresh=True)\n","    tr_loss /= step\n","    \n","    step = 0\n","    te_loss = 0\n","    rates = {'male': {'tp': 0, 'fp': 0, 'fn': 0}, \n","             'female': {'tp': 0, 'fp': 0, 'fn': 0} }\n","    with torch.no_grad():\n","        model.eval()\n","        val_pbar = tqdm(validation_dataloader, leave=False)\n","        for batch in val_pbar:\n","            step += 1\n","            images = batch[-1]\n","            \n","            images = images.to(device)\n","            \n","            logits_per_image, logits_per_text = model(images, gender_texts)\n","            ground_truth = torch.zeros((BATCH_SIZE, len(gender_labels))).to(device) # torch.arange(BATCH_SIZE).to(device)\n","            \n","            for i in range(BATCH_SIZE):\n","                truth_idx = gender_labels.index(batch[2][i])\n","                ground_truth[i, truth_idx] = 1\n","\n","            total_loss = loss_img(logits_per_image, ground_truth)\n","            te_loss += total_loss.item()\n","            \n","            update_eq_odds_rates(rates, gender_labels, logits_per_image, batch[2])\n","            \n","            val_pbar.set_description(f\"test batchCE: {total_loss.item()}\", refresh=True)\n","        te_loss /= step\n","        \n","    # Equalized odds calculation\n","    tpr_values = {label : 0 for label in rates.keys()}\n","    fpr_values = {label : 0 for label in rates.keys()}\n","    for label in rates.keys():\n","        label_rates = rates[label]\n","        tpr_values[label] = tpr(label_rates['tp'], label_rates['fn'])\n","        fpr_values[label] = fpr(label_rates['fp'], label_rates['fn'])\n","    \n","    equalized_odds = True\n","    for pair in combinations(tpr_values.keys(), 2):\n","        first_label = pair[0]\n","        second_label = pair[1]\n","        te_bias = [abs(tpr_values[first_label] - tpr_values[second_label]), abs(fpr_values[first_label] - fpr_values[second_label])]\n","        if max(te_bias[0], te_bias[1]) >= EQ_ODDS_THRESHOLD:\n","            equalized_odds = False\n","            break\n","    \n","    if te_loss < best_te_loss and equalized_odds: # maximize accuracy with fairness threshold\n","        best_te_loss = te_loss\n","        best_ep = epoch\n","        best_te_bias = te_bias\n","        torch.save(model.state_dict(), \"best_model.pt\")\n","    print(f\"epoch {epoch}, tr_loss {tr_loss}, te_loss {te_loss}, te_bias {te_bias}\")\n","\n","torch.save(model.state_dict(), \"last_model.pt\")"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"papermill":{"default_parameters":{},"duration":12893.522064,"end_time":"2021-05-17T23:31:46.031557","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-05-17T19:56:52.509493","version":"2.3.3"}},"nbformat":4,"nbformat_minor":5}
