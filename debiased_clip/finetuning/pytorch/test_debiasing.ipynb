{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "\n",
    "sys.path.append('/Users/hanselblanco/Documents/4to/ML/project/bias-project-ML')\n",
    "from clip_execution import run_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m/Users/hanselblanco/Documents/4to/ML/project/bias-project-ML/debiased_clip/finetuning/pytorch/best_model/best_model_1.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1112\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1116\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1121\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/Users/hanselblanco/Documents/4to/ML/project/bias-project-ML/debiased_clip/finetuning/pytorch/best_model/best_model_1.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 'tp'\n",
    "FP = 'fp'\n",
    "FN = 'fn'\n",
    "MALE = 'male'\n",
    "FEMALE = 'female'\n",
    "GENDER_LABELS = [MALE, FEMALE]\n",
    "GENDER_TKNS = ['This is a person of' + gender_label + 'gender.' for gender_label in GENDER_LABELS ]\n",
    "TEST_DF = pd.read_json('test_data_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_clip(['gender'], [GENDER_LABELS], [GENDER_TKNS], TEST_DF, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>predicted_gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3932</th>\n",
       "      <td>45_1_0_20170109221158465.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>9_0_0_20170110215848132.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>9</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4648</th>\n",
       "      <td>50_0_3_20170119154108905.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>50</td>\n",
       "      <td>male</td>\n",
       "      <td>indian</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3698</th>\n",
       "      <td>27_0_1_20170113000907378.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "      <td>black</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>9_1_0_20170109202813775.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>9</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>25_1_4_20170103230304689.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>25</td>\n",
       "      <td>female</td>\n",
       "      <td>other</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3035</th>\n",
       "      <td>9_0_0_20170110221716630.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>9</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>51_1_1_20170112213230359.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>51</td>\n",
       "      <td>female</td>\n",
       "      <td>black</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3510</th>\n",
       "      <td>27_0_0_20170117175613066.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3815</th>\n",
       "      <td>25_1_3_20170119171949009.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>25</td>\n",
       "      <td>female</td>\n",
       "      <td>indian</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>17_1_2_20161219190706307.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>17</td>\n",
       "      <td>female</td>\n",
       "      <td>asian</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>26_0_0_20170112205800959.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>26</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>19_1_0_20170109213228173.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>36_1_0_20170103181817489.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>36</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>26_0_0_20170116204056556.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>26</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>10_0_0_20170110225421531.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>10</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>28_1_0_20170117180716305.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>28</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>25_0_0_20170116222920399.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4394</th>\n",
       "      <td>8_0_0_20170110225315590.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>8</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>18_1_2_20170109213146944.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>18</td>\n",
       "      <td>female</td>\n",
       "      <td>asian</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   filename  \\\n",
       "3932  45_1_0_20170109221158465.jpg.chip.jpg   \n",
       "1843   9_0_0_20170110215848132.jpg.chip.jpg   \n",
       "4648  50_0_3_20170119154108905.jpg.chip.jpg   \n",
       "3698  27_0_1_20170113000907378.jpg.chip.jpg   \n",
       "1897   9_1_0_20170109202813775.jpg.chip.jpg   \n",
       "28    25_1_4_20170103230304689.jpg.chip.jpg   \n",
       "3035   9_0_0_20170110221716630.jpg.chip.jpg   \n",
       "752   51_1_1_20170112213230359.jpg.chip.jpg   \n",
       "3510  27_0_0_20170117175613066.jpg.chip.jpg   \n",
       "3815  25_1_3_20170119171949009.jpg.chip.jpg   \n",
       "975   17_1_2_20161219190706307.jpg.chip.jpg   \n",
       "812   26_0_0_20170112205800959.jpg.chip.jpg   \n",
       "803   19_1_0_20170109213228173.jpg.chip.jpg   \n",
       "3058  36_1_0_20170103181817489.jpg.chip.jpg   \n",
       "3096  26_0_0_20170116204056556.jpg.chip.jpg   \n",
       "291   10_0_0_20170110225421531.jpg.chip.jpg   \n",
       "325   28_1_0_20170117180716305.jpg.chip.jpg   \n",
       "1724  25_0_0_20170116222920399.jpg.chip.jpg   \n",
       "4394   8_0_0_20170110225315590.jpg.chip.jpg   \n",
       "2246  18_1_2_20170109213146944.jpg.chip.jpg   \n",
       "\n",
       "                                               filepath  age  gender    race  \\\n",
       "3932  /Users/hanselblanco/Documents/4to/ML/project/b...   45  female   white   \n",
       "1843  /Users/hanselblanco/Documents/4to/ML/project/b...    9    male   white   \n",
       "4648  /Users/hanselblanco/Documents/4to/ML/project/b...   50    male  indian   \n",
       "3698  /Users/hanselblanco/Documents/4to/ML/project/b...   27    male   black   \n",
       "1897  /Users/hanselblanco/Documents/4to/ML/project/b...    9  female   white   \n",
       "28    /Users/hanselblanco/Documents/4to/ML/project/b...   25  female   other   \n",
       "3035  /Users/hanselblanco/Documents/4to/ML/project/b...    9    male   white   \n",
       "752   /Users/hanselblanco/Documents/4to/ML/project/b...   51  female   black   \n",
       "3510  /Users/hanselblanco/Documents/4to/ML/project/b...   27    male   white   \n",
       "3815  /Users/hanselblanco/Documents/4to/ML/project/b...   25  female  indian   \n",
       "975   /Users/hanselblanco/Documents/4to/ML/project/b...   17  female   asian   \n",
       "812   /Users/hanselblanco/Documents/4to/ML/project/b...   26    male   white   \n",
       "803   /Users/hanselblanco/Documents/4to/ML/project/b...   19  female   white   \n",
       "3058  /Users/hanselblanco/Documents/4to/ML/project/b...   36  female   white   \n",
       "3096  /Users/hanselblanco/Documents/4to/ML/project/b...   26    male   white   \n",
       "291   /Users/hanselblanco/Documents/4to/ML/project/b...   10    male   white   \n",
       "325   /Users/hanselblanco/Documents/4to/ML/project/b...   28  female   white   \n",
       "1724  /Users/hanselblanco/Documents/4to/ML/project/b...   25    male   white   \n",
       "4394  /Users/hanselblanco/Documents/4to/ML/project/b...    8    male   white   \n",
       "2246  /Users/hanselblanco/Documents/4to/ML/project/b...   18  female   asian   \n",
       "\n",
       "     predicted_gender  \n",
       "3932           female  \n",
       "1843             male  \n",
       "4648           female  \n",
       "3698           female  \n",
       "1897           female  \n",
       "28               male  \n",
       "3035             male  \n",
       "752            female  \n",
       "3510             male  \n",
       "3815             male  \n",
       "975              male  \n",
       "812              male  \n",
       "803            female  \n",
       "3058           female  \n",
       "3096             male  \n",
       "291              male  \n",
       "325            female  \n",
       "1724             male  \n",
       "4394             male  \n",
       "2246             male  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3710"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=0\n",
    "for i in range(len(df)):\n",
    "    if(df['gender'][i]==df['predicted_gender'][i]): k+=1\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=df['gender'].tolist()\n",
    "y_pred=df['predicted_gender'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_total=df['gender'].value_counts()[MALE]\n",
    "female_total= df['gender'].value_counts()[FEMALE]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7808705817906364"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_accuracy= metrics.balanced_accuracy_score(y_true, y_pred)\n",
    "macro_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score (macro)\n",
    "\n",
    "F1 score can be interpreted as a measure of overall model performance from 0 to 1, where 1 is the best. To be more specific, F1 score can be interpreted as the model’s balanced ability to both capture positive cases (recall) and be accurate with the cases it does capture (precision).\n",
    "\n",
    "the model’s ability to both capture positive cases and be accurate with the cases it does capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7813977896448396"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_true, y_pred, labels = GENDER_LABELS, average='macro')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confusion_matrix= metrics.confusion_matrix(y_true, y_pred, labels = GENDER_LABELS)\n",
    "# metrics.plot_confusion_matrix(confusion_matrix, labels=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing FP TP FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_rates = {'male':{'tp': 0, 'fp': 0, 'fn': 0}, 'female':{'tp': 0, 'fp': 0, 'fn': 0}}\n",
    "totals = {'male': 0, 'female': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \n",
    "false_negatives = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "true_positives = np.diag(confusion_matrix)\n",
    "true_negatives = confusion_matrix.sum() - (false_positives + false_negatives + true_positives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling dictionaries\n",
    "for i in range (len(GENDER_LABELS)):\n",
    "    sex_rates[GENDER_LABELS[i]][TP]= true_positives[i]\n",
    "    sex_rates[GENDER_LABELS[i]][FP]= false_positives[i]\n",
    "    sex_rates[GENDER_LABELS[i]][FN]= false_negatives[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8179249091643117, 0.7438162544169611)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "males_sr = sex_rates[MALE][TP]/ male_total\n",
    "females_sr = sex_rates[FEMALE][TP]/ female_total\n",
    "males_sr, females_sr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8179249091643117, 0.7438162544169611)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "males_tpr = sex_rates[MALE][TP]/ (sex_rates[MALE][TP] + sex_rates[MALE][FN])\n",
    "females_tpr = sex_rates[FEMALE][TP]/ (sex_rates[FEMALE][TP] + sex_rates[FEMALE][FN])\n",
    "\n",
    "males_tpr, females_tpr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.562560620756547, 0.43743937924345294)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "males_fpr= sex_rates[MALE][FP]/(sex_rates[MALE][FP]+ sex_rates[MALE][FN])\n",
    "females_fpr= sex_rates[FEMALE][FP]/(sex_rates[FEMALE][FP]+ sex_rates[FEMALE][FN])\n",
    "\n",
    "males_fpr, females_fpr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Fairness Metrics`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalized odds\n",
      "0.07410865474735062\n"
     ]
    }
   ],
   "source": [
    "if abs(males_tpr - females_tpr) < 0.15:\n",
    "    print('Equalized odds')\n",
    "else:\n",
    "    print('Not equalized odds')\n",
    "print(abs(males_tpr - females_tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalized odds\n",
      "0.12512124151309406\n"
     ]
    }
   ],
   "source": [
    "if abs(males_fpr - females_fpr) < 0.15:\n",
    "    print('Equalized odds')\n",
    "else:\n",
    "    print('Not equalized odds')\n",
    "print(abs(males_fpr - females_fpr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disparate impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No disparate impact present in female group / male group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9093943051287328"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disparate impact ratio = underprivileged group SR / privileged group SR\n",
    "disp_impact = females_sr / males_sr\n",
    "if disp_impact < 0.8:\n",
    "    print('Disparate impact present in female group / male group')\n",
    "else:\n",
    "    print('No disparate impact present in female group / male group')\n",
    "disp_impact"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
