{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "\n",
    "sys.path.append('/Users/hanselblanco/Documents/4to/ML/project/bias-project-ML')\n",
    "from clip_execution import run_clip\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/hanselblanco/Documents/4to/ML/project/bias-project-ML/debiased_clip/finetuning/pytorch/best_model/best_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m/Users/hanselblanco/Documents/4to/ML/project/bias-project-ML/debiased_clip/finetuning/pytorch/best_model/best_model.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/hanselblanco/Documents/4to/ML/project/bias-project-ML/debiased_clip/finetuning/pytorch/best_model/best_model.pt'"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/Users/hanselblanco/Documents/4to/ML/project/bias-project-ML/debiased_clip/finetuning/pytorch/best_model/best_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MALE = 'male'\n",
    "FEMALE = 'female'\n",
    "GENDER_LABELS = [MALE, FEMALE]\n",
    "GENDER_TKNS = ['This is a person of' + gender_label + 'gender.' for gender_label in GENDER_LABELS ]\n",
    "TEST_DF = pd.read_json('test_data_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_clip(['gender'], [GENDER_LABELS], [GENDER_TKNS], TEST_DF, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>predicted_gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3932</th>\n",
       "      <td>45_1_0_20170109221158465.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>9_0_0_20170110215848132.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>9</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4648</th>\n",
       "      <td>50_0_3_20170119154108905.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>50</td>\n",
       "      <td>male</td>\n",
       "      <td>indian</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3698</th>\n",
       "      <td>27_0_1_20170113000907378.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "      <td>black</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>9_1_0_20170109202813775.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>9</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>25_1_4_20170103230304689.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>25</td>\n",
       "      <td>female</td>\n",
       "      <td>other</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3035</th>\n",
       "      <td>9_0_0_20170110221716630.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>9</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>51_1_1_20170112213230359.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>51</td>\n",
       "      <td>female</td>\n",
       "      <td>black</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3510</th>\n",
       "      <td>27_0_0_20170117175613066.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3815</th>\n",
       "      <td>25_1_3_20170119171949009.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>25</td>\n",
       "      <td>female</td>\n",
       "      <td>indian</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>17_1_2_20161219190706307.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>17</td>\n",
       "      <td>female</td>\n",
       "      <td>asian</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>26_0_0_20170112205800959.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>26</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>19_1_0_20170109213228173.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>36_1_0_20170103181817489.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>36</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>26_0_0_20170116204056556.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>26</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>10_0_0_20170110225421531.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>10</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>28_1_0_20170117180716305.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>28</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>25_0_0_20170116222920399.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4394</th>\n",
       "      <td>8_0_0_20170110225315590.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>8</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>18_1_2_20170109213146944.jpg.chip.jpg</td>\n",
       "      <td>/Users/hanselblanco/Documents/4to/ML/project/b...</td>\n",
       "      <td>18</td>\n",
       "      <td>female</td>\n",
       "      <td>asian</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   filename  \\\n",
       "3932  45_1_0_20170109221158465.jpg.chip.jpg   \n",
       "1843   9_0_0_20170110215848132.jpg.chip.jpg   \n",
       "4648  50_0_3_20170119154108905.jpg.chip.jpg   \n",
       "3698  27_0_1_20170113000907378.jpg.chip.jpg   \n",
       "1897   9_1_0_20170109202813775.jpg.chip.jpg   \n",
       "28    25_1_4_20170103230304689.jpg.chip.jpg   \n",
       "3035   9_0_0_20170110221716630.jpg.chip.jpg   \n",
       "752   51_1_1_20170112213230359.jpg.chip.jpg   \n",
       "3510  27_0_0_20170117175613066.jpg.chip.jpg   \n",
       "3815  25_1_3_20170119171949009.jpg.chip.jpg   \n",
       "975   17_1_2_20161219190706307.jpg.chip.jpg   \n",
       "812   26_0_0_20170112205800959.jpg.chip.jpg   \n",
       "803   19_1_0_20170109213228173.jpg.chip.jpg   \n",
       "3058  36_1_0_20170103181817489.jpg.chip.jpg   \n",
       "3096  26_0_0_20170116204056556.jpg.chip.jpg   \n",
       "291   10_0_0_20170110225421531.jpg.chip.jpg   \n",
       "325   28_1_0_20170117180716305.jpg.chip.jpg   \n",
       "1724  25_0_0_20170116222920399.jpg.chip.jpg   \n",
       "4394   8_0_0_20170110225315590.jpg.chip.jpg   \n",
       "2246  18_1_2_20170109213146944.jpg.chip.jpg   \n",
       "\n",
       "                                               filepath  age  gender    race  \\\n",
       "3932  /Users/hanselblanco/Documents/4to/ML/project/b...   45  female   white   \n",
       "1843  /Users/hanselblanco/Documents/4to/ML/project/b...    9    male   white   \n",
       "4648  /Users/hanselblanco/Documents/4to/ML/project/b...   50    male  indian   \n",
       "3698  /Users/hanselblanco/Documents/4to/ML/project/b...   27    male   black   \n",
       "1897  /Users/hanselblanco/Documents/4to/ML/project/b...    9  female   white   \n",
       "28    /Users/hanselblanco/Documents/4to/ML/project/b...   25  female   other   \n",
       "3035  /Users/hanselblanco/Documents/4to/ML/project/b...    9    male   white   \n",
       "752   /Users/hanselblanco/Documents/4to/ML/project/b...   51  female   black   \n",
       "3510  /Users/hanselblanco/Documents/4to/ML/project/b...   27    male   white   \n",
       "3815  /Users/hanselblanco/Documents/4to/ML/project/b...   25  female  indian   \n",
       "975   /Users/hanselblanco/Documents/4to/ML/project/b...   17  female   asian   \n",
       "812   /Users/hanselblanco/Documents/4to/ML/project/b...   26    male   white   \n",
       "803   /Users/hanselblanco/Documents/4to/ML/project/b...   19  female   white   \n",
       "3058  /Users/hanselblanco/Documents/4to/ML/project/b...   36  female   white   \n",
       "3096  /Users/hanselblanco/Documents/4to/ML/project/b...   26    male   white   \n",
       "291   /Users/hanselblanco/Documents/4to/ML/project/b...   10    male   white   \n",
       "325   /Users/hanselblanco/Documents/4to/ML/project/b...   28  female   white   \n",
       "1724  /Users/hanselblanco/Documents/4to/ML/project/b...   25    male   white   \n",
       "4394  /Users/hanselblanco/Documents/4to/ML/project/b...    8    male   white   \n",
       "2246  /Users/hanselblanco/Documents/4to/ML/project/b...   18  female   asian   \n",
       "\n",
       "     predicted_gender  \n",
       "3932           female  \n",
       "1843             male  \n",
       "4648           female  \n",
       "3698           female  \n",
       "1897           female  \n",
       "28               male  \n",
       "3035             male  \n",
       "752            female  \n",
       "3510             male  \n",
       "3815             male  \n",
       "975              male  \n",
       "812              male  \n",
       "803            female  \n",
       "3058           female  \n",
       "3096             male  \n",
       "291              male  \n",
       "325            female  \n",
       "1724             male  \n",
       "4394             male  \n",
       "2246             male  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=df['gender'].tolist()\n",
    "y_pred=df['predicted_gender'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_total=df['gender'].value_counts()[MALE]\n",
    "female_total= df['gender'].value_counts()[FEMALE]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7808705817906364"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_accuracy= macro_accuracy(y_true, y_pred)\n",
    "macro_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score (macro)\n",
    "\n",
    "F1 score can be interpreted as a measure of overall model performance from 0 to 1, where 1 is the best. To be more specific, F1 score can be interpreted as the model’s balanced ability to both capture positive cases (recall) and be accurate with the cases it does capture (precision).\n",
    "\n",
    "the model’s ability to both capture positive cases and be accurate with the cases it does capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7813977896448396"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(y_true, y_pred, labels = GENDER_LABELS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confusion_matrix = confusion_matrix(y_true, y_pred, labels = GENDER_LABELS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8179249091643117, 0.7438162544169611)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srs = selection_rate(y_true, y_pred, GENDER_LABELS)\n",
    "males_sr = srs[MALE]\n",
    "females_sr = srs[FEMALE]\n",
    "males_sr, females_sr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8179249091643117, 0.7438162544169611)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tprs = tpr(y_true, y_pred, GENDER_LABELS)\n",
    "\n",
    "males_tpr = tprs[MALE]\n",
    "females_tpr = tprs[FEMALE]\n",
    "\n",
    "males_tpr, females_tpr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.562560620756547, 0.43743937924345294)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fprs = fpr(y_true, y_pred, GENDER_LABELS)\n",
    "\n",
    "males_fpr = fprs[MALE]\n",
    "females_fpr = fprs[FEMALE]\n",
    "\n",
    "males_fpr, females_fpr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Fairness Metrics`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalized odds\n",
      "0.07410865474735062\n"
     ]
    }
   ],
   "source": [
    "if abs(males_tpr - females_tpr) < 0.15:\n",
    "    print('Equalized odds')\n",
    "else:\n",
    "    print('Not equalized odds')\n",
    "print(abs(males_tpr - females_tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalized odds\n",
      "0.12512124151309406\n"
     ]
    }
   ],
   "source": [
    "if abs(males_fpr - females_fpr) < 0.15:\n",
    "    print('Equalized odds')\n",
    "else:\n",
    "    print('Not equalized odds')\n",
    "print(abs(males_fpr - females_fpr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disparate impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No disparate impact present in female group / male group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9093943051287328"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disparate impact ratio = underprivileged group SR / privileged group SR\n",
    "disp_impact = females_sr / males_sr\n",
    "if disp_impact < 0.8:\n",
    "    print('Disparate impact present in female group / male group')\n",
    "else:\n",
    "    print('No disparate impact present in female group / male group')\n",
    "disp_impact"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
