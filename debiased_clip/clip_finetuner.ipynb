{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataframe` to `DocumentArray`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users/lachy/Videos/LACHY/SRI Repo/bias-project-ML\\data_preprocess.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['gender'][i]= gender_mapper[df['gender'][i]]\n",
      "C:\\Users/lachy/Videos/LACHY/SRI Repo/bias-project-ML\\data_preprocess.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['race'][i]= race_mapper[df['race'][i]]\n"
     ]
    }
   ],
   "source": [
    "from docarray import Document, DocumentArray\n",
    "import sys\n",
    "\n",
    "sys.path.append('C:/Users/lachy/Videos/LACHY/SRI Repo/bias-project-ML')\n",
    "from data_preprocess import data_selection\n",
    "\n",
    "train_df, test_df = data_selection('C:/Users/lachy/Pictures/utkface/utkface/')\n",
    "\n",
    "train_da = DocumentArray.from_dataframe(train_df)\n",
    "test_da = DocumentArray.from_dataframe(test_df)\n",
    "\n",
    "# for item in train_da:\n",
    "#     item.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing `CLIP`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CLIP` Fine-tuning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269dafce18ae45299cfb2f8bcee98751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"\\n<div class='custom-container'>\\n    <style>\\n        .custom-container {\\n       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import finetuner\n",
    "\n",
    "finetuner.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Race ` and ` Gender` Fine-tuning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">📄 <span style=\"font-weight: bold\">Document</span>: <span style=\"color: #008080; text-decoration-color: #008080\">c1a2d528259f7f9e7b832045f1e996ae</span>\n",
       "└── 💠 <span style=\"font-weight: bold\">Chunks</span>\n",
       "    ├── 📄 <span style=\"font-weight: bold\">Document</span>: <span style=\"color: #008080; text-decoration-color: #008080\">8033cda3e333ea306d1d7bd96e017ebc</span>\n",
       "    │   ╭───────────┬──────────────────────────────────────────────────────────────────╮\n",
       "    │   │<span style=\"font-weight: bold\"> Attribute </span>│<span style=\"font-weight: bold\"> Value                                                            </span>│\n",
       "    │   ├───────────┼──────────────────────────────────────────────────────────────────┤\n",
       "    │   │ tensor    │ <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'numpy.ndarray'</span><span style=\"font-weight: bold\">&gt;</span> in shape <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">224</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">224</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>, dtype: uint8     │\n",
       "    │   │ mime_type │ image/jpeg                                                       │\n",
       "    │   │ uri       │ C:/Users/lachy/Pictures/utkface/utkface/100_0_0_201701122152403… │\n",
       "    │   │ tags      │ {'filepath':                                                     │\n",
       "    │   │           │ 'C:/Users/lachy/Pictures/utkface/utkface/100_0_0_20170112215240… │\n",
       "    │   │           │ 'race': 'white', 'gender': 'male', 'age': 100, 'filename':       │\n",
       "    │   │           │ '100_0_0_20170112215240346.jpg.chip.jpg'}                        │\n",
       "    │   │ modality  │ image                                                            │\n",
       "    │   ╰───────────┴──────────────────────────────────────────────────────────────────╯\n",
       "    └── 📄 <span style=\"font-weight: bold\">Document</span>: <span style=\"color: #008080; text-decoration-color: #008080\">111fd89056dff9bb8c10a45e20842446</span>\n",
       "        ╭───────────────┬──────────────────────────────────────────────────────────────╮\n",
       "        │<span style=\"font-weight: bold\"> Attribute     </span>│<span style=\"font-weight: bold\"> Value                                                        </span>│\n",
       "        ├───────────────┼──────────────────────────────────────────────────────────────┤\n",
       "        │ text          │ This is a person of male gender and white race.              │\n",
       "        │ modality      │ text                                                         │\n",
       "        ╰───────────────┴──────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "📄 \u001b[1mDocument\u001b[0m: \u001b[36mc1a2d528259f7f9e7b832045f1e996ae\u001b[0m\n",
       "└── 💠 \u001b[1mChunks\u001b[0m\n",
       "    ├── 📄 \u001b[1mDocument\u001b[0m: \u001b[36m8033cda3e333ea306d1d7bd96e017ebc\u001b[0m\n",
       "    │   ╭───────────┬──────────────────────────────────────────────────────────────────╮\n",
       "    │   │\u001b[1m \u001b[0m\u001b[1mAttribute\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                                                           \u001b[0m\u001b[1m \u001b[0m│\n",
       "    │   ├───────────┼──────────────────────────────────────────────────────────────────┤\n",
       "    │   │ tensor    │ \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'numpy.ndarray'\u001b[0m\u001b[1m>\u001b[0m in shape \u001b[1m(\u001b[0m\u001b[1;36m224\u001b[0m, \u001b[1;36m224\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, dtype: uint8     │\n",
       "    │   │ mime_type │ image/jpeg                                                       │\n",
       "    │   │ uri       │ C:/Users/lachy/Pictures/utkface/utkface/100_0_0_201701122152403… │\n",
       "    │   │ tags      │ {'filepath':                                                     │\n",
       "    │   │           │ 'C:/Users/lachy/Pictures/utkface/utkface/100_0_0_20170112215240… │\n",
       "    │   │           │ 'race': 'white', 'gender': 'male', 'age': 100, 'filename':       │\n",
       "    │   │           │ '100_0_0_20170112215240346.jpg.chip.jpg'}                        │\n",
       "    │   │ modality  │ image                                                            │\n",
       "    │   ╰───────────┴──────────────────────────────────────────────────────────────────╯\n",
       "    └── 📄 \u001b[1mDocument\u001b[0m: \u001b[36m111fd89056dff9bb8c10a45e20842446\u001b[0m\n",
       "        ╭───────────────┬──────────────────────────────────────────────────────────────╮\n",
       "        │\u001b[1m \u001b[0m\u001b[1mAttribute    \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                                                       \u001b[0m\u001b[1m \u001b[0m│\n",
       "        ├───────────────┼──────────────────────────────────────────────────────────────┤\n",
       "        │ text          │ This is a person of male gender and white race.              │\n",
       "        │ modality      │ text                                                         │\n",
       "        ╰───────────────┴──────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gender_pairs = DocumentArray() # initialize a DocumentArray as final training data.\n",
    "\n",
    "prompt = 'This is a person of '\n",
    "for doc in train_da:\n",
    "    pair = Document()\n",
    "    doc.uri = doc.tags['filepath']\n",
    "    img_chunk = doc.load_uri_to_image_tensor(224, 224)\n",
    "    img_chunk.modality = 'image'\n",
    "    txt_chunk = Document(content=prompt + doc.tags['gender'] + ' gender and ' + doc.tags['race'] + ' race.' )\n",
    "    txt_chunk.modality = 'text'\n",
    "    pair.chunks.extend([img_chunk, txt_chunk])\n",
    "    # add pair to pairs\n",
    "    gender_pairs.append(pair)\n",
    "\n",
    "# Lets see the first item of the pairs\n",
    "gender_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18966"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gender_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training `CLIP` model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing a DocumentArray to Hubble under the name finetuner-dastorage-default-elated-easley-train ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48eb484220ec4c6b8faa4fc66b7f8aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = finetuner.fit(\n",
    "    model='openai/clip-vit-base-patch32', # fine-tune CLIP\n",
    "    train_data=gender_pairs,   \n",
    "    learning_rate=1e-5,\n",
    "    loss='CLIPLoss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2f8158e33440a1a4e975c23e7cfb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:13:12] INFO     Starting finetuner training run ...                                                  __main__.py:350\n",
      "DEBUG    Found Jina AI Cloud authentication token                                             __main__.py:362\n",
      "DEBUG    Running in online mode                                                               __main__.py:363\n",
      "INFO     Reading config ...                                                                   __main__.py:370\n",
      "DEBUG    Reading config from stream                                                           __main__.py:382\n",
      "INFO     Parsing config ...                                                                   __main__.py:385\n",
      "INFO     Config loaded 📜                                                                     __main__.py:389\n",
      "INFO     Run name: elated-easley                                                              __main__.py:391\n",
      "INFO     Experiment name: default                                                             __main__.py:392\n",
      "DEBUG    Device set to [cuda]                                                                 __main__.py:395\n",
      "DEBUG    Artifact ID set to 646d9b3dc52fb1a3083d6643                                          __main__.py:141\n",
      "DEBUG    Artifact ID for metrics set to 646d9b3dc52fb1a3083d6644                              __main__.py:144\n",
      "DEBUG    Artifact ID for example results set to 646d9b3ec52fb1a3083d6645                      __main__.py:147\n",
      "INFO     Building the tuner components ...                                                    __main__.py:161\n",
      "INFO     Building models ...                                                                    tuning.py:320\n",
      "DEBUG    Model name: openai/clip-vit-base-patch32                                               tuning.py:322\n",
      "DEBUG    Model options: {}                                                                      tuning.py:323\n",
      "Downloading:   0%|          | 0.00/4.09k [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 4.09k/4.09k [00:00<00:00, 5.25MB/s]\n",
      "Downloading:   0%|          | 0.00/577M [00:00<?, ?B/s]\n",
      "Downloading:   1%|▏         | 7.30M/577M [00:00<00:07, 76.6MB/s]\n",
      "Downloading:   3%|▎         | 15.2M/577M [00:00<00:07, 80.3MB/s]\n",
      "Downloading:   4%|▍         | 23.1M/577M [00:00<00:07, 81.6MB/s]\n",
      "Downloading:   5%|▌         | 31.0M/577M [00:00<00:06, 82.0MB/s]\n",
      "Downloading:   7%|▋         | 39.0M/577M [00:00<00:06, 82.5MB/s]\n",
      "Downloading:   8%|▊         | 46.8M/577M [00:00<00:06, 82.5MB/s]\n",
      "Downloading:   9%|▉         | 54.8M/577M [00:00<00:06, 82.6MB/s]\n",
      "Downloading:  11%|█         | 62.7M/577M [00:00<00:06, 82.8MB/s]\n",
      "Downloading:  12%|█▏        | 70.6M/577M [00:00<00:06, 82.8MB/s]\n",
      "Downloading:  14%|█▎        | 78.5M/577M [00:01<00:06, 82.5MB/s]\n",
      "Downloading:  15%|█▍        | 86.4M/577M [00:01<00:06, 82.8MB/s]\n",
      "Downloading:  16%|█▋        | 94.4M/577M [00:01<00:06, 83.0MB/s]\n",
      "Downloading:  18%|█▊        | 102M/577M [00:01<00:06, 83.0MB/s] \n",
      "Downloading:  19%|█▉        | 110M/577M [00:01<00:05, 83.2MB/s]\n",
      "Downloading:  20%|██        | 118M/577M [00:01<00:05, 83.2MB/s]\n",
      "Downloading:  22%|██▏       | 126M/577M [00:01<00:05, 83.1MB/s]\n",
      "Downloading:  23%|██▎       | 134M/577M [00:01<00:05, 83.2MB/s]\n",
      "Downloading:  25%|██▍       | 142M/577M [00:01<00:05, 83.2MB/s]\n",
      "Downloading:  26%|██▌       | 150M/577M [00:01<00:05, 83.1MB/s]\n",
      "Downloading:  27%|██▋       | 158M/577M [00:02<00:05, 83.2MB/s]\n",
      "Downloading:  29%|██▊       | 166M/577M [00:02<00:05, 83.2MB/s]\n",
      "Downloading:  30%|███       | 174M/577M [00:02<00:05, 82.8MB/s]\n",
      "Downloading:  31%|███▏      | 182M/577M [00:02<00:05, 82.6MB/s]\n",
      "Downloading:  33%|███▎      | 190M/577M [00:02<00:04, 83.0MB/s]\n",
      "Downloading:  34%|███▍      | 198M/577M [00:02<00:04, 83.2MB/s]\n",
      "Downloading:  36%|███▌      | 206M/577M [00:02<00:04, 83.3MB/s]\n",
      "Downloading:  37%|███▋      | 214M/577M [00:02<00:04, 83.3MB/s]\n",
      "Downloading:  38%|███▊      | 222M/577M [00:02<00:04, 83.3MB/s]\n",
      "Downloading:  40%|███▉      | 230M/577M [00:02<00:04, 83.4MB/s]\n",
      "Downloading:  41%|████      | 237M/577M [00:03<00:04, 83.3MB/s]\n",
      "Downloading:  43%|████▎     | 245M/577M [00:03<00:04, 83.4MB/s]\n",
      "Downloading:  44%|████▍     | 253M/577M [00:03<00:04, 83.3MB/s]\n",
      "Downloading:  45%|████▌     | 261M/577M [00:03<00:03, 83.3MB/s]\n",
      "Downloading:  47%|████▋     | 269M/577M [00:03<00:03, 83.4MB/s]\n",
      "Downloading:  48%|████▊     | 277M/577M [00:03<00:03, 83.3MB/s]\n",
      "Downloading:  49%|████▉     | 285M/577M [00:03<00:03, 83.4MB/s]\n",
      "Downloading:  51%|█████     | 293M/577M [00:03<00:03, 83.4MB/s]\n",
      "Downloading:  52%|█████▏    | 301M/577M [00:03<00:03, 83.3MB/s]\n",
      "Downloading:  54%|█████▎    | 309M/577M [00:03<00:03, 83.3MB/s]\n",
      "Downloading:  55%|█████▍    | 317M/577M [00:04<00:03, 83.1MB/s]\n",
      "Downloading:  56%|█████▋    | 325M/577M [00:04<00:03, 83.3MB/s]\n",
      "Downloading:  58%|█████▊    | 333M/577M [00:04<00:03, 83.4MB/s]\n",
      "Downloading:  59%|█████▉    | 341M/577M [00:04<00:02, 83.5MB/s]\n",
      "Downloading:  60%|██████    | 349M/577M [00:04<00:02, 83.4MB/s]\n",
      "Downloading:  62%|██████▏   | 357M/577M [00:04<00:02, 83.3MB/s]\n",
      "Downloading:  63%|██████▎   | 365M/577M [00:04<00:02, 83.3MB/s]\n",
      "Downloading:  65%|██████▍   | 373M/577M [00:04<00:02, 83.4MB/s]\n",
      "Downloading:  66%|██████▌   | 381M/577M [00:04<00:02, 83.4MB/s]\n",
      "Downloading:  67%|██████▋   | 389M/577M [00:04<00:02, 83.2MB/s]\n",
      "Downloading:  69%|██████▊   | 397M/577M [00:05<00:02, 83.2MB/s]\n",
      "Downloading:  70%|███████   | 405M/577M [00:05<00:02, 83.3MB/s]\n",
      "Downloading:  71%|███████▏  | 413M/577M [00:05<00:02, 83.3MB/s]\n",
      "Downloading:  73%|███████▎  | 420M/577M [00:05<00:01, 83.3MB/s]\n",
      "Downloading:  74%|███████▍  | 428M/577M [00:05<00:01, 83.2MB/s]\n",
      "Downloading:  76%|███████▌  | 436M/577M [00:05<00:01, 83.2MB/s]\n",
      "Downloading:  77%|███████▋  | 444M/577M [00:05<00:01, 83.2MB/s]\n",
      "Downloading:  78%|███████▊  | 452M/577M [00:05<00:01, 83.0MB/s]\n",
      "Downloading:  80%|███████▉  | 460M/577M [00:05<00:01, 82.9MB/s]\n",
      "Downloading:  81%|████████  | 468M/577M [00:05<00:01, 82.9MB/s]\n",
      "Downloading:  82%|████████▏ | 476M/577M [00:06<00:01, 82.9MB/s]\n",
      "Downloading:  84%|████████▍ | 484M/577M [00:06<00:01, 83.0MB/s]\n",
      "Downloading:  85%|████████▌ | 492M/577M [00:06<00:01, 83.1MB/s]\n",
      "Downloading:  87%|████████▋ | 500M/577M [00:06<00:00, 82.1MB/s]\n",
      "Downloading:  88%|████████▊ | 508M/577M [00:06<00:00, 82.1MB/s]\n",
      "Downloading:  89%|████████▉ | 516M/577M [00:06<00:00, 82.5MB/s]\n",
      "Downloading:  91%|█████████ | 523M/577M [00:06<00:00, 81.8MB/s]\n",
      "Downloading:  92%|█████████▏| 531M/577M [00:06<00:00, 82.1MB/s]\n",
      "Downloading:  93%|█████████▎| 539M/577M [00:06<00:00, 82.5MB/s]\n",
      "Downloading:  95%|█████████▍| 547M/577M [00:06<00:00, 82.7MB/s]\n",
      "Downloading:  96%|█████████▌| 555M/577M [00:07<00:00, 82.8MB/s]\n",
      "Downloading:  98%|█████████▊| 563M/577M [00:07<00:00, 82.9MB/s]\n",
      "Downloading:  99%|█████████▉| 571M/577M [00:07<00:00, 83.0MB/s]\n",
      "Downloading: 100%|██████████| 577M/577M [00:07<00:00, 83.0MB/s]\n",
      "Downloading:   0%|          | 0.00/568 [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 568/568 [00:00<00:00, 1.07MB/s]\n",
      "Downloading:   0%|          | 0.00/842k [00:00<?, ?B/s]\n",
      "Downloading:  56%|█████▌    | 468k/842k [00:00<00:00, 2.67MB/s]\n",
      "Downloading: 100%|██████████| 842k/842k [00:00<00:00, 4.68MB/s]\n",
      "Downloading:   0%|          | 0.00/512k [00:00<?, ?B/s]\n",
      "Downloading:  17%|█▋        | 88.0k/512k [00:00<00:00, 499kB/s]\n",
      "Downloading: 100%|██████████| 512k/512k [00:00<00:00, 1.91MB/s]\n",
      "Downloading:   0%|          | 0.00/2.12M [00:00<?, ?B/s]\n",
      "Downloading:   4%|▍         | 86.0k/2.12M [00:00<00:04, 492kB/s]\n",
      "Downloading:  24%|██▎       | 512k/2.12M [00:00<00:01, 1.63MB/s]\n",
      "Downloading:  86%|████████▌ | 1.82M/2.12M [00:00<00:00, 5.54MB/s]\n",
      "Downloading: 100%|██████████| 2.12M/2.12M [00:00<00:00, 4.80MB/s]\n",
      "Downloading:   0%|          | 0.00/389 [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 389/389 [00:00<00:00, 749kB/s]\n",
      "ping\n",
      "2023-05-24 05:13:37.268029\n",
      "Downloading:   0%|          | 0.00/316 [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 316/316 [00:00<00:00, 479kB/s]\n",
      "[05:13:26] INFO     Loading the training data ...                                                          tuning.py:152\n",
      "INFO     Pulling data from cloud storage ...                                                      utils.py:80\n",
      "🔐 You are logged in to Jina AI as lachycastro9 (username: lachycastro9). To log out, use jina auth logout.\n",
      "ping\n",
      "2023-05-24 05:13:52.267600\n",
      "ping\n",
      "2023-05-24 05:14:07.267603\n",
      "ping\n",
      "2023-05-24 05:14:22.267780\n",
      "ping\n",
      "2023-05-24 05:14:37.267821\n",
      "ping\n",
      "2023-05-24 05:14:52.268182\n",
      "ping\n",
      "2023-05-24 05:15:07.268182\n",
      "ping\n",
      "2023-05-24 05:15:22.267500\n",
      "ping\n",
      "2023-05-24 05:15:37.268199\n",
      "ping\n",
      "2023-05-24 05:15:52.267753\n",
      "ping\n",
      "2023-05-24 05:16:07.267599\n",
      "ping\n",
      "2023-05-24 05:16:22.268205\n",
      "ping\n",
      "2023-05-24 05:16:37.268165\n",
      "ping\n",
      "2023-05-24 05:16:52.267750\n",
      "ping\n",
      "2023-05-24 05:17:07.268326\n",
      "ping\n",
      "2023-05-24 05:17:22.269227\n",
      "ping\n",
      "2023-05-24 05:17:37.270056\n",
      "ping\n",
      "2023-05-24 05:17:52.270005\n",
      "ping\n",
      "2023-05-24 05:18:07.270414\n",
      "ping\n",
      "2023-05-24 05:18:22.270961\n",
      "ping\n",
      "2023-05-24 05:18:37.270558\n",
      "ping\n",
      "2023-05-24 05:18:52.270644\n",
      "ping\n",
      "2023-05-24 05:19:07.270941\n",
      "ping\n",
      "2023-05-24 05:19:22.270561\n",
      "ping\n",
      "2023-05-24 05:19:37.270816\n",
      "ping\n",
      "2023-05-24 05:19:52.270836\n",
      "ping\n",
      "2023-05-24 05:20:07.271064\n",
      "ping\n",
      "2023-05-24 05:20:22.270635\n",
      "ping\n",
      "2023-05-24 05:20:37.270917\n",
      "ping\n",
      "2023-05-24 05:20:52.270736\n",
      "ping\n",
      "2023-05-24 05:21:07.271396\n",
      "ping\n",
      "2023-05-24 05:21:22.272359\n",
      "ping\n",
      "2023-05-24 05:21:37.272625\n",
      "ping\n",
      "2023-05-24 05:21:52.308670\n",
      "ping\n",
      "2023-05-24 05:22:07.309188\n",
      "ping\n",
      "2023-05-24 05:22:22.309017\n",
      "ping\n",
      "2023-05-24 05:22:37.309039\n",
      "ping\n",
      "2023-05-24 05:22:52.308518\n",
      "ping\n",
      "2023-05-24 05:23:07.308581\n",
      "ping\n",
      "2023-05-24 05:23:22.309337\n",
      "ping\n",
      "2023-05-24 05:23:37.310040\n",
      "ping\n",
      "2023-05-24 05:23:52.309701\n",
      "ping\n",
      "2023-05-24 05:24:07.310136\n",
      "ping\n",
      "2023-05-24 05:24:22.309446\n",
      "ping\n",
      "2023-05-24 05:24:37.309639\n",
      "[05:24:29] DEBUG    # training samples: 18966                                                              tuning.py:165\n",
      "INFO     Building loss ...                                                                      tuning.py:425\n",
      "DEBUG    Loss name: CLIPLoss                                                                    tuning.py:426\n",
      "DEBUG    Loss options: {}                                                                       tuning.py:427\n",
      "[05:24:32] DEBUG    Saving initial state of the model                                                    batchsize.py:81\n",
      "DEBUG    Testing batch size of 4                                                             batchsize.py:106\n",
      "[05:24:35] DEBUG    Testing batch size of 8                                                             batchsize.py:106\n",
      "DEBUG    Testing batch size of 16                                                            batchsize.py:106\n",
      "[05:24:36] DEBUG    Testing batch size of 32                                                            batchsize.py:106\n",
      "[05:24:37] DEBUG    Testing batch size of 64                                                            batchsize.py:106\n",
      "[05:24:38] DEBUG    Testing batch size of 128                                                           batchsize.py:106\n",
      "[05:24:40] DEBUG    Testing batch size of 256                                                           batchsize.py:106\n",
      "[05:24:42] DEBUG    Testing batch size of 512                                                           batchsize.py:106\n",
      "ping\n",
      "2023-05-24 05:24:52.310229\n",
      "[05:24:45] DEBUG    Batch_size adjusted from 4 to 256                                                    batchsize.py:86\n",
      "DEBUG    Restoring initial state of the model                                                 batchsize.py:88\n",
      "INFO     Building loss ...                                                                      tuning.py:425\n",
      "DEBUG    Loss name: CLIPLoss                                                                    tuning.py:426\n",
      "DEBUG    Loss options: {}                                                                       tuning.py:427\n",
      "INFO     Building optimizer ...                                                                 tuning.py:532\n",
      "DEBUG    Optimizer name: Adam                                                                   tuning.py:533\n",
      "DEBUG    Optimizer options: {'lr': 1e-05}                                                       tuning.py:534\n",
      "INFO     Model, data, callbacks, loss, miner and optimizer built successfully 🔥              __main__.py:165\n",
      "INFO     Finetuning ...                                                                       __main__.py:170\n",
      "ping\n",
      "2023-05-24 05:25:07.310590\n",
      "ping\n",
      "2023-05-24 05:25:22.311342\n",
      "ping\n",
      "2023-05-24 05:25:37.312336\n",
      "ping\n",
      "2023-05-24 05:25:52.312435\n",
      "ping\n",
      "2023-05-24 05:26:07.313332\n",
      "ping\n",
      "2023-05-24 05:26:22.313508\n",
      "ping\n",
      "2023-05-24 05:26:37.314041\n",
      "ping\n",
      "2023-05-24 05:26:52.313993\n",
      "ping\n",
      "2023-05-24 05:27:07.314258\n",
      "ping\n",
      "2023-05-24 05:27:22.314688\n",
      "ping\n",
      "2023-05-24 05:27:37.315471\n",
      "ping\n",
      "2023-05-24 05:27:52.316086\n",
      "ping\n",
      "2023-05-24 05:28:07.315591\n",
      "ping\n",
      "2023-05-24 05:28:22.316117\n",
      "ping\n",
      "2023-05-24 05:28:37.315576\n",
      "ping\n",
      "2023-05-24 05:28:52.315895\n",
      "ping\n",
      "2023-05-24 05:29:07.315471\n",
      "Training [5/5] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148/148 0:00:00 0:00:53 • loss: 3.606\n",
      "[05:29:12] INFO     Done ✨                                                                              __main__.py:192\n",
      "DEBUG    Finetuning took 0 days, 0 hours 4 minutes and 26 seconds                             __main__.py:194\n",
      "INFO     Building the artifact ...                                                            __main__.py:231\n",
      "INFO     Pushing artifact to Jina AI Cloud ...                                                __main__.py:260\n",
      "ping\n",
      "2023-05-24 05:29:22.316209\n",
      "ping\n",
      "2023-05-24 05:29:37.316238\n",
      "ping\n",
      "2023-05-24 05:29:52.317362\n",
      "ping\n",
      "2023-05-24 05:30:07.317549\n",
      "ping\n",
      "2023-05-24 05:30:22.317605\n",
      "ping\n",
      "2023-05-24 05:30:37.318206\n",
      "ping\n",
      "2023-05-24 05:30:52.317517\n",
      "ping\n",
      "2023-05-24 05:31:07.318159\n",
      "[05:30:57] INFO     Artifact pushed under ID '646d9b3dc52fb1a3083d6643'                                  __main__.py:266\n",
      "DEBUG    Artifact size is 535.389 MB                                                          __main__.py:268\n",
      "INFO     Finished 🚀                                                                          __main__.py:415\n",
      "time=\"2023-05-24T05:31:00.227Z\" level=info msg=\"sub-process exited\" argo=true error=\"<nil>\"\n"
     ]
    }
   ],
   "source": [
    "for entry in run.stream_logs():\n",
    "    print(entry)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m artifact \u001b[39m=\u001b[39m run\u001b[39m.\u001b[39msave_artifact(\u001b[39m'\u001b[39m\u001b[39m/artifact/\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run' is not defined"
     ]
    }
   ],
   "source": [
    "artifact = run.save_artifact('/artifact/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving encoders..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_text_encoder = finetuner.get_model(artifact='/artifact/elated-easley.zip', select_model='clip-text')\n",
    "clip_image_encoder = finetuner.get_model(artifact='/artifact/elated-easley.zip', select_model='clip-vision')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving images embeddings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a6ec38651b4529bf39b9b3ba3b86c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for d in test_da:\n",
    "    d.uri = d.tags['filepath']\n",
    "images = finetuner.encode(model=clip_image_encoder, data=test_da)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create query texts to evaluate the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A photo of a person of black race.',\n",
       " 'A photo of a person of white race.',\n",
       " 'A photo of a person of asian race.',\n",
       " 'A photo of a person of indian race.',\n",
       " 'A photo of a person of other race.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_labels = ['black', 'white', 'asian', 'indian', 'other']\n",
    "race_tks = []\n",
    "for label in race_labels:\n",
    "    race_tks.append('A photo of a person of ' + label + ' race.')\n",
    "\n",
    "sex_labels = ['male', 'female']\n",
    "sex_tks = []\n",
    "for label in sex_labels:\n",
    "    sex_tks.append('A photo of a person of ' + label + ' gender.')\n",
    "\n",
    "sex_tks\n",
    "race_tks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving texts embeddings...(sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">📄 <span style=\"font-weight: bold\">Document</span>: <span style=\"color: #008080; text-decoration-color: #008080\">2869bb3a91c69e657e5a455ab69e7f85</span>\n",
       "╭──────────────────┬───────────────────────────────────────────────────────────╮\n",
       "│<span style=\"font-weight: bold\"> Attribute        </span>│<span style=\"font-weight: bold\"> Value                                                     </span>│\n",
       "├──────────────────┼───────────────────────────────────────────────────────────┤\n",
       "│ text             │ A photo of a person of male gender.                       │\n",
       "╰──────────────────┴───────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "📄 \u001b[1mDocument\u001b[0m: \u001b[36m2869bb3a91c69e657e5a455ab69e7f85\u001b[0m\n",
       "╭──────────────────┬───────────────────────────────────────────────────────────╮\n",
       "│\u001b[1m \u001b[0m\u001b[1mAttribute       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                                                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "├──────────────────┼───────────────────────────────────────────────────────────┤\n",
       "│ text             │ A photo of a person of male gender.                       │\n",
       "╰──────────────────┴───────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">📄 <span style=\"font-weight: bold\">Document</span>: <span style=\"color: #008080; text-decoration-color: #008080\">e35c5ed55951f6c736855b2ccc33d273</span>\n",
       "╭─────────────────┬────────────────────────────────────────────────────────────╮\n",
       "│<span style=\"font-weight: bold\"> Attribute       </span>│<span style=\"font-weight: bold\"> Value                                                      </span>│\n",
       "├─────────────────┼────────────────────────────────────────────────────────────┤\n",
       "│ text            │ A photo of a person of female gender.                      │\n",
       "╰─────────────────┴────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "📄 \u001b[1mDocument\u001b[0m: \u001b[36me35c5ed55951f6c736855b2ccc33d273\u001b[0m\n",
       "╭─────────────────┬────────────────────────────────────────────────────────────╮\n",
       "│\u001b[1m \u001b[0m\u001b[1mAttribute      \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                                                     \u001b[0m\u001b[1m \u001b[0m│\n",
       "├─────────────────┼────────────────────────────────────────────────────────────┤\n",
       "│ text            │ A photo of a person of female gender.                      │\n",
       "╰─────────────────┴────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b319c4b92e7d4cdfa75c780ea105ae79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────────────────── Documents Summary ───────────────────╮\n",
       "│                                                        │\n",
       "│   Type                   DocumentArrayInMemory         │\n",
       "│   Length                 <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                             │\n",
       "│   Homogenous Documents   <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>                          │\n",
       "│   Common Attributes      <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'embedding'</span><span style=\"font-weight: bold\">)</span>   │\n",
       "│   Multimodal dataclass   <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>                         │\n",
       "│                                                        │\n",
       "╰────────────────────────────────────────────────────────╯\n",
       "╭────────────────────── Attributes Summary ───────────────────────╮\n",
       "│                                                                 │\n",
       "│  <span style=\"font-weight: bold\"> Attribute </span> <span style=\"font-weight: bold\"> Data type    </span> <span style=\"font-weight: bold\"> #Unique values </span> <span style=\"font-weight: bold\"> Has empty value </span>  │\n",
       "│  ─────────────────────────────────────────────────────────────  │\n",
       "│   embedding   <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'ndarray'</span>,<span style=\"font-weight: bold\">)</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>             │\n",
       "│   id          <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span>,<span style=\"font-weight: bold\">)</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>             │\n",
       "│   text        <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span>,<span style=\"font-weight: bold\">)</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>             │\n",
       "│                                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭────────────────── Documents Summary ───────────────────╮\n",
       "│                                                        │\n",
       "│   Type                   DocumentArrayInMemory         │\n",
       "│   Length                 \u001b[1;36m2\u001b[0m                             │\n",
       "│   Homogenous Documents   \u001b[3;92mTrue\u001b[0m                          │\n",
       "│   Common Attributes      \u001b[1m(\u001b[0m\u001b[32m'id'\u001b[0m, \u001b[32m'text'\u001b[0m, \u001b[32m'embedding'\u001b[0m\u001b[1m)\u001b[0m   │\n",
       "│   Multimodal dataclass   \u001b[3;91mFalse\u001b[0m                         │\n",
       "│                                                        │\n",
       "╰────────────────────────────────────────────────────────╯\n",
       "╭────────────────────── Attributes Summary ───────────────────────╮\n",
       "│                                                                 │\n",
       "│  \u001b[1m \u001b[0m\u001b[1mAttribute\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mData type   \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m#Unique values\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mHas empty value\u001b[0m\u001b[1m \u001b[0m  │\n",
       "│  ─────────────────────────────────────────────────────────────  │\n",
       "│   embedding   \u001b[1m(\u001b[0m\u001b[32m'ndarray'\u001b[0m,\u001b[1m)\u001b[0m   \u001b[1;36m2\u001b[0m                \u001b[3;91mFalse\u001b[0m             │\n",
       "│   id          \u001b[1m(\u001b[0m\u001b[32m'str'\u001b[0m,\u001b[1m)\u001b[0m       \u001b[1;36m2\u001b[0m                \u001b[3;91mFalse\u001b[0m             │\n",
       "│   text        \u001b[1m(\u001b[0m\u001b[32m'str'\u001b[0m,\u001b[1m)\u001b[0m       \u001b[1;36m2\u001b[0m                \u001b[3;91mFalse\u001b[0m             │\n",
       "│                                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_docs = DocumentArray()\n",
    "for item in sex_tks:\n",
    "    text_docs.append(Document(content=item))\n",
    "text_docs[0].summary()\n",
    "text_docs[1].summary()\n",
    "\n",
    "finetuner.encode(model=clip_text_encoder, data=text_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.match(text_docs, metric='cosine', limit=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting bias in finetuned `CLIP` results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex (Disparate impact)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Rate (Positive results / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9826472962066183, 0.997348652231551)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_males, positive_females = 0, 0\n",
    "\n",
    "total_males, total_females = 0, 0\n",
    "\n",
    "for i in range(len(images)):\n",
    "   if images[i].tags['gender'] == 'male':\n",
    "        total_males += 1\n",
    "        if images[i].matches[0].text.split(' ')[-2] == 'male':\n",
    "            positive_males += 1\n",
    "   else:\n",
    "        total_females += 1\n",
    "        if images[i].matches[0].text.split(' ')[-2] == 'female':\n",
    "            positive_females += 1\n",
    "                \n",
    "males_sr, females_sr = positive_males/ total_males, positive_females/ total_females\n",
    "\n",
    "males_sr, females_sr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disparate impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0149609692935455"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# disparate impact ratio = underprivileged group SR / privileged group SR\n",
    "disp_impact = females_sr / males_sr\n",
    "disp_impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if disp_impact < 0.8:\n",
    "    print('Disparate impact present in female group / male group')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex (Equalized odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_males, tp_females, fn_males, fn_females, fp_males, fp_females = 0, 0, 0, 0, 0, 0\n",
    "ages_of_fp=[]\n",
    "for i in range(len(images)):\n",
    "    if images[i].tags['gender'] == 'male':\n",
    "        if images[i].matches[0].text.split(' ')[-2] == 'male':\n",
    "            tp_males += 1\n",
    "        else:\n",
    "            fp_females += 1\n",
    "            ages_of_fp.append(images[i].tags['age'])\n",
    "            fn_males += 1 # False negative (wrong no male prediction, in this case, equal to female false positive)\n",
    "    elif images[i].tags['gender'] == 'female':\n",
    "        if images[i].matches[0].text.split(' ')[-2] == 'female':\n",
    "            tp_females += 1\n",
    "        else:\n",
    "            fp_males += 1\n",
    "            ages_of_fp.append(images[i].tags['age'])\n",
    "            fn_females += 1\n",
    "                \n",
    "males_tpr, females_tpr = tp_males/ (tp_males + fn_males), tp_females/ (tp_females + fn_females)\n",
    "\n",
    "males_fpr, females_fpr = fp_males/ (fp_males + fn_males), fp_females/ (fp_females + fn_females)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True Positive Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9826472962066183, 0.997348652231551)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "males_tpr, females_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalized odds\n"
     ]
    }
   ],
   "source": [
    "if abs(males_tpr - females_tpr) < 0.05:\n",
    "    print('Equalized odds')\n",
    "else:\n",
    "    print('Not equalized odds')\n",
    "    print(abs(males_tpr - females_tpr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Positive Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12244897959183673, 0.8775510204081632)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "males_fpr, females_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not equalized odds\n",
      "0.7551020408163265\n"
     ]
    }
   ],
   "source": [
    "if abs(males_fpr - females_fpr) < 0.05:\n",
    "    print('Equalized odds')\n",
    "else:\n",
    "    print('Not equalized odds')\n",
    "    print(abs(males_fpr - females_fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/lachy/Pictures/utkface/utkface/10_0_0_20170110220251986.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/11_0_0_20170110220500946.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/15_0_0_20170104011743800.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/15_0_3_20170104225254497.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/16_0_0_20170110231736665.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/1_0_2_20161219200015132.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/1_0_3_20161219224956400.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/1_0_3_20161219225615528.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/1_0_3_20161220223100579.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/1_0_4_20161221192548675.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/1_1_0_20170109191027883.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/21_0_2_20170116170638620.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/21_0_4_20161223214826657.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/21_0_4_20161223214831392.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/21_0_4_20161223214835183.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/21_0_4_20161223214837442.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/23_0_1_20170117194105675.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/24_0_2_20170116164749805.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/24_0_2_20170116171647508.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/25_0_4_20170117151414412.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/26_0_0_20170113210127413.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/26_0_3_20170119181302068.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/26_0_4_20170117200550222.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/26_1_2_20170116182219997.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/27_0_2_20170116172745659.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/28_0_0_20170116194200664.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/28_1_1_20170113150922335.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/29_0_2_20170116165939507.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/2_1_4_20170103210746899.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/30_0_4_20170117202929576.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/33_0_0_20170117182541126.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/34_0_0_20170104191711254.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/35_0_1_20170109001203061.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/35_0_3_20170119200913517.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/36_0_0_20170117183240678.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/36_0_3_20170119180245724.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/36_1_3_20170117183408380.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/38_0_3_20170119202607101.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/42_0_0_20170104184335702.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/50_1_3_20170117154120315.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/54_0_0_20170113210127075.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/55_0_0_20170116222002492.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/55_0_0_20170116232628323.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/57_0_4_20170105171850037.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/70_0_0_20170116213553686.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/70_0_0_20170120223312220.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/70_0_1_20170117194145056.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/83_0_0_20170111211106903.jpg.chip.jpg',\n",
       " 'C:/Users/lachy/Pictures/utkface/utkface/90_0_0_20170111224014347.jpg.chip.jpg']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_class = []\n",
    "for i in range(len(images)):\n",
    "    if  images[i].tags['gender'] != images[i].matches[0].text.split(' ')[-2]:\n",
    "        wrong_class.append(images[i].tags['filepath'])\n",
    "\n",
    "wrong_class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruido en los datos:\n",
    "Datos mal etiquetados:\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/55_0_0_20170116232628323.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/54_0_0_20170113210127075.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/50_1_3_20170117154120315.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/38_0_3_20170119202607101.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/36_1_3_20170117183408380.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/36_0_3_20170119180245724.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/36_0_0_20170117183240678.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/35_0_1_20170109001203061.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/33_0_0_20170117182541126.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/30_0_4_20170117202929576.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/27_0_2_20170116172745659.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/26_1_2_20170116182219997.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/26_0_4_20170117200550222.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/26_0_3_20170119181302068.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/26_0_0_20170113210127413.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/25_0_4_20170117151414412.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/24_0_2_20170116164749805.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/21_0_4_20161223214837442.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/21_0_4_20161223214835183.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/21_0_4_20161223214826657.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/21_0_4_20161223214831392.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/21_0_2_20170116170638620.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/1_0_3_20161220223100579.jpg.chip.jpg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datos dificiles:\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/10_0_0_20170110220251986.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/55_0_0_20170116222002492.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/42_0_0_20170104184335702.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/2_1_4_20170103210746899.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/28_1_1_20170113150922335.jpg.chip.jpg'\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/1_1_0_20170109191027883.jpg.chip.jpg\n",
    "'C:/Users/lachy/Pictures/utkface/utkface/1_0_4_20161221192548675.jpg.chip.jpg'\n",
    "C:/Users/lachy/Pictures/utkface/utkface/1_0_3_20161219225615528.jpg.chip.jpg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Race (Disparate impact)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving texts embeddings...(race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">📄 <span style=\"font-weight: bold\">Document</span>: <span style=\"color: #008080; text-decoration-color: #008080\">c01a09444f2784494cd1a0c52e9adb1a</span>\n",
       "╭───────────────────┬──────────────────────────────────────────────────────────╮\n",
       "│<span style=\"font-weight: bold\"> Attribute         </span>│<span style=\"font-weight: bold\"> Value                                                    </span>│\n",
       "├───────────────────┼──────────────────────────────────────────────────────────┤\n",
       "│ text              │ A photo of a person of black race.                       │\n",
       "╰───────────────────┴──────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "📄 \u001b[1mDocument\u001b[0m: \u001b[36mc01a09444f2784494cd1a0c52e9adb1a\u001b[0m\n",
       "╭───────────────────┬──────────────────────────────────────────────────────────╮\n",
       "│\u001b[1m \u001b[0m\u001b[1mAttribute        \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                                                   \u001b[0m\u001b[1m \u001b[0m│\n",
       "├───────────────────┼──────────────────────────────────────────────────────────┤\n",
       "│ text              │ A photo of a person of black race.                       │\n",
       "╰───────────────────┴──────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">📄 <span style=\"font-weight: bold\">Document</span>: <span style=\"color: #008080; text-decoration-color: #008080\">22ca394f41f2455fbb32dc9f241470ae</span>\n",
       "╭───────────────────┬──────────────────────────────────────────────────────────╮\n",
       "│<span style=\"font-weight: bold\"> Attribute         </span>│<span style=\"font-weight: bold\"> Value                                                    </span>│\n",
       "├───────────────────┼──────────────────────────────────────────────────────────┤\n",
       "│ text              │ A photo of a person of white race.                       │\n",
       "╰───────────────────┴──────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "📄 \u001b[1mDocument\u001b[0m: \u001b[36m22ca394f41f2455fbb32dc9f241470ae\u001b[0m\n",
       "╭───────────────────┬──────────────────────────────────────────────────────────╮\n",
       "│\u001b[1m \u001b[0m\u001b[1mAttribute        \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                                                   \u001b[0m\u001b[1m \u001b[0m│\n",
       "├───────────────────┼──────────────────────────────────────────────────────────┤\n",
       "│ text              │ A photo of a person of white race.                       │\n",
       "╰───────────────────┴──────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b533cd93cc46d1ac2ba72855657b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────────────────── Documents Summary ───────────────────╮\n",
       "│                                                        │\n",
       "│   Type                   DocumentArrayInMemory         │\n",
       "│   Length                 <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>                             │\n",
       "│   Homogenous Documents   <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>                          │\n",
       "│   Common Attributes      <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'embedding'</span><span style=\"font-weight: bold\">)</span>   │\n",
       "│   Multimodal dataclass   <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>                         │\n",
       "│                                                        │\n",
       "╰────────────────────────────────────────────────────────╯\n",
       "╭────────────────────── Attributes Summary ───────────────────────╮\n",
       "│                                                                 │\n",
       "│  <span style=\"font-weight: bold\"> Attribute </span> <span style=\"font-weight: bold\"> Data type    </span> <span style=\"font-weight: bold\"> #Unique values </span> <span style=\"font-weight: bold\"> Has empty value </span>  │\n",
       "│  ─────────────────────────────────────────────────────────────  │\n",
       "│   embedding   <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'ndarray'</span>,<span style=\"font-weight: bold\">)</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>                <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>             │\n",
       "│   id          <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span>,<span style=\"font-weight: bold\">)</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>                <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>             │\n",
       "│   text        <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span>,<span style=\"font-weight: bold\">)</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>                <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>             │\n",
       "│                                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭────────────────── Documents Summary ───────────────────╮\n",
       "│                                                        │\n",
       "│   Type                   DocumentArrayInMemory         │\n",
       "│   Length                 \u001b[1;36m5\u001b[0m                             │\n",
       "│   Homogenous Documents   \u001b[3;92mTrue\u001b[0m                          │\n",
       "│   Common Attributes      \u001b[1m(\u001b[0m\u001b[32m'id'\u001b[0m, \u001b[32m'text'\u001b[0m, \u001b[32m'embedding'\u001b[0m\u001b[1m)\u001b[0m   │\n",
       "│   Multimodal dataclass   \u001b[3;91mFalse\u001b[0m                         │\n",
       "│                                                        │\n",
       "╰────────────────────────────────────────────────────────╯\n",
       "╭────────────────────── Attributes Summary ───────────────────────╮\n",
       "│                                                                 │\n",
       "│  \u001b[1m \u001b[0m\u001b[1mAttribute\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mData type   \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m#Unique values\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mHas empty value\u001b[0m\u001b[1m \u001b[0m  │\n",
       "│  ─────────────────────────────────────────────────────────────  │\n",
       "│   embedding   \u001b[1m(\u001b[0m\u001b[32m'ndarray'\u001b[0m,\u001b[1m)\u001b[0m   \u001b[1;36m5\u001b[0m                \u001b[3;91mFalse\u001b[0m             │\n",
       "│   id          \u001b[1m(\u001b[0m\u001b[32m'str'\u001b[0m,\u001b[1m)\u001b[0m       \u001b[1;36m5\u001b[0m                \u001b[3;91mFalse\u001b[0m             │\n",
       "│   text        \u001b[1m(\u001b[0m\u001b[32m'str'\u001b[0m,\u001b[1m)\u001b[0m       \u001b[1;36m5\u001b[0m                \u001b[3;91mFalse\u001b[0m             │\n",
       "│                                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_docs = DocumentArray()\n",
    "for item in race_tks:\n",
    "    text_docs.append(Document(content=item))\n",
    "text_docs[0].summary()\n",
    "text_docs[1].summary()\n",
    "\n",
    "finetuner.encode(model=clip_text_encoder, data=text_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.match(text_docs, metric='cosine', limit=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Rate (Positive results / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9409429280397023,\n",
       " 0.9900662251655629,\n",
       " 0.9781659388646288,\n",
       " 0.9459119496855346)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_whites, positive_blacks, positive_asians, positive_indians = 0, 0, 0, 0\n",
    "\n",
    "total_whites, total_blacks, total_asians, total_indians = 0, 0, 0, 0\n",
    "\n",
    "for i in range(len(images)):\n",
    "    if images[i].tags['race']== 'white':\n",
    "        total_whites += 1\n",
    "        if images[i].matches[0].text.split(' ')[-2] == 'white':\n",
    "            positive_whites += 1\n",
    "    elif images[i].tags['race'] == 'black':\n",
    "        total_blacks += 1\n",
    "        if images[i].matches[0].text.split(' ')[-2] == 'black':\n",
    "            positive_blacks += 1\n",
    "    elif images[i].tags['race'] == 'asian':\n",
    "        total_asians += 1\n",
    "        if images[i].matches[0].text.split(' ')[-2] == 'asian':\n",
    "            positive_asians += 1\n",
    "    elif images[i].tags['race'] == 'indian':\n",
    "        total_indians += 1\n",
    "        if images[i].matches[0].text.split(' ')[-2] == 'indian':\n",
    "            positive_indians += 1\n",
    "    else:\n",
    "        continue\n",
    "                \n",
    "whites_sr, blacks_sr, asians_sr, indians_sr = positive_whites/ total_whites, positive_blacks/ total_blacks, positive_asians/ total_asians, positive_indians/ total_indians\n",
    "\n",
    "whites_sr, blacks_sr, asians_sr, indians_sr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disparate impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0505442041646178, 1.037916991475633, 1.0050274722545134)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disparate impact ratio = underprivileged group SR / privileged group SR\n",
    "disp_impact_b_w = blacks_sr/ whites_sr\n",
    "disp_impact_b_a = asians_sr/ whites_sr\n",
    "disp_impact_b_i = indians_sr/ whites_sr\n",
    "disp_impact_b_w, disp_impact_b_a, disp_impact_b_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if disp_impact_b_w < 0.8:\n",
    "    print('Disparate impact present in black group / white group')\n",
    "if disp_impact_b_a < 0.8:\n",
    "    print('Disparate impact present in asian group / white group')\n",
    "if disp_impact_b_i < 0.8:\n",
    "    print('Disparate impact present in indian group / white group')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Race (Equalized odds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_rates = {'white': {'tp': 0, 'fp': 0, 'fn': 0}, 'black': {'tp': 0, 'fp': 0, 'fn': 0}, 'asian': {'tp': 0, 'fp': 0, 'fn': 0}, 'indian': {'tp': 0, 'fp': 0, 'fn': 0}, 'other': {'tp': 0, 'fp': 0, 'fn': 0}}\n",
    "\n",
    "for i in range(len(images)):\n",
    "    race = images[i].tags['race']\n",
    "    pred_race = images[i].matches[0].text.split(' ')[-2]\n",
    "\n",
    "    if race == pred_race:\n",
    "        race_rates[race]['tp'] += 1\n",
    "    else:\n",
    "        race_rates[race]['fn'] += 1\n",
    "        race_rates[pred_race]['fp'] += 1\n",
    "\n",
    "tpr = lambda tp, fn: tp/ (tp + fn)\n",
    "fpr = lambda fp, fn: fp/ (fp + fn)\n",
    "\n",
    "tpr_values = {'white': 0, 'black': 0, 'asian': 0, 'indian': 0, 'other': 0}\n",
    "fpr_values = {'white': 0, 'black': 0, 'asian': 0, 'indian': 0, 'other': 0}\n",
    "for race in race_rates.keys():\n",
    "    rates = race_rates[race]\n",
    "    tpr_values[race] = tpr(rates['tp'], rates['fn'])\n",
    "    fpr_values[race] = fpr(rates['fp'], rates['fn'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Positive Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9409429280397023,\n",
       " 0.9900662251655629,\n",
       " 0.9781659388646288,\n",
       " 0.9459119496855346,\n",
       " 0.9171597633136095]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tpr_value for tpr_value in tpr_values.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "equalized_odds = True\n",
    "\n",
    "for pair in combinations(tpr_values.keys(), 2):\n",
    "    first_race = pair[0]\n",
    "    second_race = pair[1]\n",
    "    if first_race == 'other' or second_race == 'other':\n",
    "        continue\n",
    "    if abs(tpr_values[first_race] - tpr_values[second_race]) >= 0.05:\n",
    "        equalized_odds = False\n",
    "        print('Not equalized odds between ' + first_race + ' and ' + second_race)\n",
    "\n",
    "if equalized_odds:\n",
    "    print('Equalized odds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
