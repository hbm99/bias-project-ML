{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataframe` to `DocumentArray`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docarray import Document, DocumentArray\n",
    "import pandas\n",
    "\n",
    "from k_fold import data_selection\n",
    "\n",
    "train_df, test_df = data_selection('/Users/hanselblanco/Documents/4to/ML/project/bias-project-ML/datasets/utkface')\n",
    "\n",
    "train_da = DocumentArray.from_dataframe(train_df)\n",
    "test_da = DocumentArray.from_dataframe(test_df)\n",
    "\n",
    "# for item in train_da:\n",
    "#     item.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing `CLIP`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CLIP` Fine-tuning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finetuner\n",
    "\n",
    "finetuner.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Race` Fine-tuning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_pairs = DocumentArray() # initialize a DocumentArray as final training data.\n",
    "\n",
    "prompt = 'This is a person of race '\n",
    "for doc in train_da:\n",
    "    pair = Document()\n",
    "    doc.uri = doc.tags['filepath']\n",
    "    img_chunk = doc.load_uri_to_image_tensor(224, 224)\n",
    "    img_chunk.modality = 'image'\n",
    "    txt_chunk = Document(content=prompt + doc.tags['race'])\n",
    "    txt_chunk.modality = 'text'\n",
    "    pair.chunks.extend([img_chunk, txt_chunk])\n",
    "    # add pair to pairs\n",
    "    race_pairs.append(pair)\n",
    "\n",
    "# Lets see the first item of the pairs\n",
    "race_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(race_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Gender` Fine-tuning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_pairs = DocumentArray() # initialize a DocumentArray as final training data.\n",
    "\n",
    "prompt = 'This is a person of gender '\n",
    "for doc in train_da:\n",
    "    pair = Document()\n",
    "    doc.uri = doc.tags['filepath']\n",
    "    img_chunk = doc.load_uri_to_image_tensor(224, 224)\n",
    "    img_chunk.modality = 'image'\n",
    "    txt_chunk = Document(content=prompt + doc.tags['gender'])\n",
    "    txt_chunk.modality = 'text'\n",
    "    pair.chunks.extend([img_chunk, txt_chunk])\n",
    "    # add pair to pairs\n",
    "    gender_pairs.append(pair)\n",
    "\n",
    "# Lets see the first item of the pairs\n",
    "gender_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gender_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training `CLIP` model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = finetuner.fit(\n",
    "    model='openai/clip-vit-base-patch32', # fine-tune CLIP\n",
    "    train_data=race_pairs,   \n",
    "    learning_rate=1e-5,\n",
    "    loss='CLIPLoss',\n",
    "    cpu=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in run.stream_logs():\n",
    "    print(entry)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = run.save_artifact('/content/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_text_encoder = finetuner.get_model(artifact=artifact, select_model='clip-text')\n",
    "clip_image_encoder = finetuner.get_model(artifact=artifact, select_model='clip-vision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
