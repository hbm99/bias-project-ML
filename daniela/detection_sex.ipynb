{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"0bbc0f3fd3cac3ba420b59ffab6248f589b2080c"},"source":["## Notebook: Image Recognition - Gender Detection\n","\n","* Este conjunto de datos en Kaggle no permite trabajar con el método \"flow_from_directory\" de keras, ya que las imágenes deben estar en diferentes carpetas según el objetivo y el tipo de datos (entrenamiento, validación, prueba). Para esto estoy trabajando con el método \"flow\".\n","* Limitación de memoria, por lo que estoy usando una cantidad reducida de imágenes para entrenar y validar.\n","\n","### Now let´s start\n","\n","En este proyecto, construiremos un algoritmo de aprendizaje automático usando CNN para predecir a partir de una imagen si la celebridad es hombre o mujer.\n","\n","---\n","\n","## Dataset\n","\n","Para este proyecto, utilizaremos el conjunto de datos de CelebA (http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html), que está disponible en Kaggle.\n","\n","Descripción del conjunto de datos CelebA de kaggle (https://www.kaggle.com/jessicali9530/celeba-dataset):\n","### Context\n","Estos datos fueron recopilados originalmente por investigadores de MMLAB, la Universidad China de Hong Kong (referencia específica en la sección de Agradecimientos).\n","\n","### Content\n","\n","#### Overall\n","\n","202,599 número de imágenes de rostros de varias celebridades\n","10,177 identidades únicas, pero no se dan los nombres de las identidades\n","40 anotaciones de atributos binarios por imagen\n","5 lugares emblemáticos\n","\n","#### Data Files\n","\n","-img_align_celeba.zip: Todas las imágenes de rostros, recortadas y alineadas\n","\n","-list_eval_partition.csv: partición recomendada de imágenes en conjuntos de entrenamiento, validación y prueba. Las imágenes 1-162770 son de entrenamiento, 162771-182637 son de validación, 182638-202599 son de prueba\n","\n","-list_bbox_celeba.csv: información del cuadro delimitador para cada imagen. \"x_1\" e \"y_1\" representan la coordenada del punto superior izquierdo del cuadro delimitador. \"ancho\" y \"alto\" representan el ancho y el alto del cuadro delimitador\n","\n","-list_landmarks_align_celeba.csv: Imagen de puntos de referencia y sus respectivas coordenadas. Hay 5 puntos de referencia: ojo izquierdo, ojo derecho, nariz, boca izquierda, boca derecha\n","\n","-list_attr_celeba.csv: Etiquetas de atributos para cada imagen. Hay 40 atributos. \"1\" representa positivo mientras que \"-1\" representa negativo\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"_uuid":"cba9cc1ea22168a0da81d39181ef5da1951cb1f4"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"49d886506abe4d6d3ec2729036966a0729ee56d8","trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import cv2    \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import f1_score\n","\n","from keras.applications.inception_v3 import InceptionV3, preprocess_input\n","from keras import optimizers\n","from keras.models import Sequential, Model \n","from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n","from keras.utils import np_utils\n","from keras.optimizers import SGD\n","\n","from IPython.core.display import display, HTML\n","from PIL import Image\n","from io import BytesIO\n","import base64\n","\n","plt.style.use('ggplot')\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"86450cb83d348d839f1bfa14f2655166fe626407","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"352244740550c3cfb275c496add9c71bc9865b41"},"source":["## Step 1: Data Exploration\n","\n","Usaremos el CelebA Dataset, que incluye imágenes de 178 x 218 px. A continuación se muestra un ejemplo de cómo se ven las imágenes."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c269fd3d69a5b8ffaad47d56a85c785d6d7be5c1","trusted":true},"outputs":[],"source":["# set variables \n","main_folder = '../input/celeba-dataset/'\n","images_folder = main_folder + 'img_align_celeba/img_align_celeba/'\n","\n","EXAMPLE_PIC = images_folder + '000506.jpg'\n","\n","TRAINING_SAMPLES = 10000\n","VALIDATION_SAMPLES = 2000\n","TEST_SAMPLES = 2000\n","IMG_WIDTH = 178\n","IMG_HEIGHT = 218\n","BATCH_SIZE = 16\n","NUM_EPOCHS = 20"]},{"cell_type":"markdown","metadata":{"_uuid":"d4effe59d9137ff22c55ae1f331772e4e728fb5e"},"source":["### Load the attributes of every picture\n","File: list_attr_celeba.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1a6e65c380d3050be07f88488f495e34c76fc860","trusted":true},"outputs":[],"source":["# import the data set that include the attribute for each picture\n","df_attr = pd.read_csv(main_folder + 'list_attr_celeba.csv')\n","df_attr.set_index('image_id', inplace=True)\n","df_attr.replace(to_replace=-1, value=0, inplace=True) #replace -1 by 0\n","df_attr.shape"]},{"cell_type":"markdown","metadata":{"_uuid":"3b67c72c9f346f0cdb1901e0b8b79ef37a5468bc"},"source":["### List of the available attribute in the CelebA dataset\n","\n","40 Attributes"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2b79520895583694e69eef65aeadad065ffe4839","trusted":true},"outputs":[],"source":["# List of available attributes\n","for i, j in enumerate(df_attr.columns):\n","    print(i, j)"]},{"cell_type":"markdown","metadata":{"_uuid":"809cda0aead0c34725c4516a5e39aa1a9ad40c6c"},"source":["### Example of a picture in CelebA dataset\n","178 x 218 px"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c0acd2a32edb087315588d3fb9a7cf13477cdbcc","trusted":true},"outputs":[],"source":["# plot picture and attributes\n","img = load_img(EXAMPLE_PIC)\n","plt.grid(False)\n","plt.imshow(img)\n","df_attr.loc[EXAMPLE_PIC.split('/')[-1]][['Smiling','Male','Young']] #some attributes"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"31337d0dbbed4ba6009884b8688860bab8b42c18"},"source":["### Distribution of the Attribute\n","\n","Como se especificó anteriormente, este es un proyecto imaginario de reconocimiento del Género. Hay más sexo femenino que masculino en el conjunto de datos. Esto nos da una idea de la necesidad de equilibrar los datos en los próximos pasos."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2153575008289555fe054216eeb0c946f5d34555","trusted":true},"outputs":[],"source":["# Female or Male?\n","plt.title('Female or Male')\n","sns.countplot(y='Male', data=df_attr, color=\"c\")\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"ba04abfd3a6f51409bdfec909afcfd44c67421aa"},"source":["## Step 2: Split Dataset into Training, Validation and Test\n","\n","La partición recomendada de imágenes en entrenamiento, validación, prueba del conjunto de datos es:\n","* 1-162770 de entrenando\n","* 162771-182637 de validación\n","* 182638-202599 de tester\n","\n","La partición está en el archivo <b>list_eval_partition.csv</b>\n","\n","Debido al tiempo de ejecución, a estas alturas estaremos utilizando un número reducido de imágenes:\n","\n","* Entrenamiento 20000 imágenes\n","* Validación 5000 imágenes\n","* Prueba 5000 Imágenes"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"60fd112d3dfb79ae7ecf6963d74919f6d44d7c76","trusted":true},"outputs":[],"source":["# Recomended partition\n","df_partition = pd.read_csv(main_folder + 'list_eval_partition.csv')\n","df_partition.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e1328dcf8b461e037de24c18cd528791766c8f57","trusted":true},"outputs":[],"source":["# display counter by partition\n","# 0 -> TRAINING\n","# 1 -> VALIDATION\n","# 2 -> TEST\n","df_partition['partition'].value_counts().sort_index()"]},{"cell_type":"markdown","metadata":{"_uuid":"bbb655b199e34c8dbee371b67eda6c75e26ad1e1"},"source":["#### Join the partition and the attributes in the same data frame"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2e03c64bb8d57dcf39de372fabcf78e5c6c39985","trusted":true},"outputs":[],"source":["# join the partition with the attributes\n","df_partition.set_index('image_id', inplace=True)\n","df_par_attr = df_partition.join(df_attr['Male'], how='inner')\n","df_par_attr.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"6becbeba1d5fefc4416e041cef5f5d701dacc4aa"},"source":["### 2.1: Generate Partitions (Train, Validation, Test)\n","\n","Es necesario equilibrar el número de imágenes para obtener un buen rendimiento del modelo; cada modelo tendrá su propia carpeta de datos de entrenamiento, validación y prueba equilibrada.\n","\n","Este proyecto de grado explica cómo los datos de entrenamiento desequilibrados impactan en los modelos de CNN:\n","\n","https://www.kth.se/social/files/588617ebf2765401cfcc478c/PHensmanDMasko_dkand15.pdf\n","\n","En este paso crearemos funciones que nos ayudarán a crear cada partición."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"56b4af1f2f980957659c0272b92c1f77c13162c4","trusted":true},"outputs":[],"source":["def load_reshape_img(fname):\n","    img = load_img(fname)\n","    x = img_to_array(img)/255.\n","    x = x.reshape((1,) + x.shape)\n","\n","    return x\n","\n","\n","def generate_df(partition, attr, num_samples):\n","    '''\n","    partition\n","        0 -> train\n","        1 -> validation\n","        2 -> test\n","    \n","    '''\n","    \n","    df_ = df_par_attr[(df_par_attr['partition'] == partition) \n","                           & (df_par_attr[attr] == 0)].sample(int(num_samples/2))\n","    df_ = pd.concat([df_,\n","                      df_par_attr[(df_par_attr['partition'] == partition) \n","                                  & (df_par_attr[attr] == 1)].sample(int(num_samples/2))])\n","\n","    # for Train and Validation\n","    if partition != 2:\n","        x_ = np.array([load_reshape_img(images_folder + fname) for fname in df_.index])\n","        x_ = x_.reshape(x_.shape[0], 218, 178, 3)\n","        y_ = np_utils.to_categorical(df_[attr],2)\n","    # for Test\n","    else:\n","        x_ = []\n","        y_ = []\n","\n","        for index, target in df_.iterrows():\n","            im = cv2.imread(images_folder + index)\n","            im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (IMG_WIDTH, IMG_HEIGHT)).astype(np.float32) / 255.0\n","            im = np.expand_dims(im, axis =0)\n","            x_.append(im)\n","            y_.append(target[attr])\n","\n","    return x_, y_"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"659ad807ced4e62a900c31fb5d53ae81be550880"},"source":["## Step 3: Pre-processing Images: Data Augmentation\n","\n","Generates Data Augmentation for iamges.\n","\n","Data Augmentation permite generar imágenes con modificaciones a las originales. El modelo aprenderá de estas variaciones (cambio de ángulo, tamaño y posición), pudiendo predecir mejor imágenes nunca vistas que podrían tener las mismas variaciones en posición, tamaño y posición."]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"19a23bdb3f3f9542aa4743d5ae249ef3b4b57d3b"},"source":["### 3.1. Let's start with an example: Data Augmentation\n","\n","Así es como se verá una imagen después del aumento de datos (basado en los parámetros proporcionados a continuación)."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"6a26680b19401c291da42a6d59f894cef40c92f7","trusted":true},"outputs":[],"source":["# Generate image generator for data augmentation\n","datagen =  ImageDataGenerator(\n","  #preprocessing_function=preprocess_input,\n","  rotation_range=30,\n","  width_shift_range=0.2,\n","  height_shift_range=0.2,\n","  shear_range=0.2,\n","  zoom_range=0.2,\n","  horizontal_flip=True\n",")\n","\n","# load one image and reshape\n","img = load_img(EXAMPLE_PIC)\n","x = img_to_array(img)/255.\n","x = x.reshape((1,) + x.shape)\n","\n","# plot 10 augmented images of the loaded iamge\n","plt.figure(figsize=(20,10))\n","plt.suptitle('Data Augmentation', fontsize=28)\n","\n","i = 0\n","for batch in datagen.flow(x, batch_size=1):\n","    plt.subplot(3, 5, i+1)\n","    plt.grid(False)\n","    plt.imshow( batch.reshape(218, 178, 3))\n","    \n","    if i == 9:\n","        break\n","    i += 1\n","    \n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"6072f6676eb6fee3084da3df56d8e4a3f55a9868"},"source":["El resultado es un nuevo conjunto de imágenes con modificaciones del original, que permite al modelo aprender de estas variaciones para tomar este tipo de imágenes durante el proceso de aprendizaje y predecir mejor las imágenes nunca vistas."]},{"cell_type":"markdown","metadata":{"_uuid":"768e425237481c6bb6b980272fe63137bbbc20df"},"source":["### 3.2. Build Data Generators"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"35bbdcb65be9f69fc0ed02e28c946d4419a4335b","trusted":true},"outputs":[],"source":["# Train data\n","x_train, y_train = generate_df(0, 'Male', TRAINING_SAMPLES)\n","\n","# Train - Data Preparation - Data Augmentation with generators\n","train_datagen =  ImageDataGenerator(\n","  preprocessing_function=preprocess_input,\n","  rotation_range=30,\n","  width_shift_range=0.2,\n","  height_shift_range=0.2,\n","  shear_range=0.2,\n","  zoom_range=0.2,\n","  horizontal_flip=True,\n",")\n","\n","train_datagen.fit(x_train)\n","\n","train_generator = train_datagen.flow(\n","x_train, y_train,\n","batch_size=BATCH_SIZE,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2dbaf8b9a50fc5e524436fb3c2202f6e0053c86a","trusted":true},"outputs":[],"source":["# Validation Data\n","x_valid, y_valid = generate_df(1, 'Male', VALIDATION_SAMPLES)\n","\n","'''\n","# Validation - Data Preparation - Data Augmentation with generators\n","valid_datagen = ImageDataGenerator(\n","  preprocessing_function=preprocess_input,\n",")\n","\n","valid_datagen.fit(x_valid)\n","\n","validation_generator = valid_datagen.flow(\n","x_valid, y_valid,\n",")\n","'''"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"5496743ce168a108a84e7f21b60f3f58309bbaa0"},"source":["Con el generador de datos creado y los datos para la validación, estamos listos para comenzar a modelar."]},{"cell_type":"markdown","metadata":{"_uuid":"c729337fbac35a302c96a49faaf122dac082c8c2"},"source":["## Step 4: Build the Model - Gender Recognition"]},{"cell_type":"markdown","metadata":{"_uuid":"8cd7789c59348ff27b6556bda4338d51715e4c87"},"source":["### 4.1. Set the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"7e0dbc3b3d11876f7a3dc36e1f67ac093978f526","trusted":true},"outputs":[],"source":["# Import InceptionV3 Model\n","inc_model = InceptionV3(weights='../input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n","                        include_top=False,\n","                        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n","\n","print(\"number of layers:\", len(inc_model.layers))\n","#inc_model.summary()"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"ad941871e156206d4dee61879e357e808748fece"},"source":["<h2>Inception-V3 model structure</h2>\n","Esta es la estructura del modelo Inception-V3, desarrollado sobre el conjunto de datos imagenet.\n","\n","<img src=\"https://i.imgur.com/kdXUzu1.png\" width=\"1000px\"/>\n","source: https://hackathonprojects.files.wordpress.com/2016/09/74911-image03.png\n","\n","Las capas superiores (incluida la clasificación) no están incluidas. Estas capas serán reemplazadas por las siguientes capas:"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"232d3dc90e0e8859c596e6113b5dca8571c84d78","trusted":true},"outputs":[],"source":["#Adding custom Layers\n","x = inc_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation=\"relu\")(x)\n","x = Dropout(0.5)(x)\n","x = Dense(512, activation=\"relu\")(x)\n","predictions = Dense(2, activation=\"softmax\")(x)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"5644135b7f94522ff2699373d7efe539b36e07f9"},"source":["<h2>New Top layers</h2>\n","Capas a entrenar con el nuevo modelo.\n","<img src=\"https://i.imgur.com/rWF7bRY.png\" width=\"800px\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3d24a13d3e0944d5dcf008fe6f9c85ec4e702a01","trusted":true},"outputs":[],"source":["# creating the final model \n","model_ = Model(inputs=inc_model.input, outputs=predictions)\n","\n","# Lock initial layers to do not be trained\n","for layer in model_.layers[:52]:\n","    layer.trainable = False\n","\n","# compile the model\n","model_.compile(optimizer=SGD(lr=0.0001, momentum=0.9)\n","                    , loss='categorical_crossentropy'\n","                    , metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"_uuid":"0c84c56204fc441e54ed51b47e967a02cf4a5041"},"source":["### 4.2. Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b16d04820bd6ff6dd7bc24fbe51a7cc12ae47eec","trusted":true},"outputs":[],"source":["#https://keras.io/models/sequential/ fit generator\n","checkpointer = ModelCheckpoint(filepath='weights.best.inc.male.hdf5', \n","                               verbose=1, save_best_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a8808fbd97d7158400cbe1677e78ff30bb2b12a9","scrolled":true,"trusted":true},"outputs":[],"source":["hist = model_.fit_generator(train_generator\n","                     , validation_data = (x_valid, y_valid)\n","                      , steps_per_epoch= TRAINING_SAMPLES/BATCH_SIZE\n","                      , epochs= NUM_EPOCHS\n","                      , callbacks=[checkpointer]\n","                      , verbose=1\n","                    )"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"72e75a244fdf073d8f1278fa525de6c8ccf9a41e"},"source":["#### El mejor modelo después de NUM_epech obtuvo una precisión sobre los datos de validación del 95,75 %."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ed704e5742f2d73c80641e8a957ba9fd5354c4a9","trusted":true},"outputs":[],"source":["# Plot loss function value through epochs\n","plt.figure(figsize=(18, 4))\n","plt.plot(hist.history['loss'], label = 'train')\n","plt.plot(hist.history['val_loss'], label = 'valid')\n","plt.legend()\n","plt.title('Loss Function')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"052c31f101f2e8bb92a0a871bffb7ef35881266f","trusted":true},"outputs":[],"source":["# Plot accuracy through epochs\n","plt.figure(figsize=(18, 4))\n","plt.plot(hist.history['acc'], label = 'train')\n","plt.plot(hist.history['val_acc'], label = 'valid')\n","plt.legend()\n","plt.title('Accuracy')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"56f81f1620f273e93e2d194752d6b309308bf29e"},"source":["### 4.3. Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"61f890047aa886fa8942867ad76d3a73104b1360","trusted":true},"outputs":[],"source":["#load the best model\n","model_.load_weights('weights.best.inc.male.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e77ebcbefdd61f27f0cfe02a13f30134ea3921d5","trusted":true},"outputs":[],"source":["# Test Data\n","x_test, y_test = generate_df(2, 'Male', TEST_SAMPLES)\n","\n","# generate prediction\n","model_predictions = [np.argmax(model_.predict(feature)) for feature in x_test ]\n","\n","# report test accuracy\n","test_accuracy = 100 * np.sum(np.array(model_predictions)==y_test) / len(model_predictions)\n","print('Model Evaluation')\n","print('Test accuracy: %.4f%%' % test_accuracy)\n","print('f1_score:', f1_score(y_test, model_predictions))"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"9efc0a6de2ff07268a946d897b6615c26e795e97"},"source":["### 5. Conclusion\n","\n","El modelo construido que utiliza el aprendizaje de transferencia de InceptionV3 y agrega capas personalizadas reconoce con éxito el género que brinda cierta imagen con <b>94.8% de precisión sobre los datos de prueba</b>. No obstante, se detectan algunas limitaciones y oportunidades de mejora:\n","\n","* Entrena los algoritmos con todo el conjunto de datos de imágenes. Debido a la limitación de recursos computacionales, el modelo se entrenó con un subconjunto de imágenes. Contando con una máquina adecuada, se puede entrenar el modelo incluyendo todas las imágenes. Esto hará que el algoritmo aprenda de diferentes contextos de la imagen, dándole más experiencia para predecir mejores imágenes nunca vistas.\n","\n","* Usar estructuras de diferencias para las CNN. Este enfoque podría brindar un mejor rendimiento al modelo; de todos modos, es una tarea costosa, ya que el modelo se puede medir en el conjunto de datos de prueba después de entrenarlo, y esto requiere tiempo y recursos computacionales.\n","\n","* Al ver las imágenes del conjunto de datos de CelebA, la mayoría de las imágenes son casi un primer plano de la cara del sujeto, lo que lleva al modelo a aprender de este tipo de imágenes, y en situaciones en las que el sujeto es solo un pequeño parte de una imagen, el modelo no podría funcionar bien. Para lidiar con esto, se pueden agregar datos de preprocesamiento más sofisticados o complementar el conjunto de datos con imágenes que no se basan completamente en primeros planos de la cara del sujeto.\n","\n","* Los entornos donde hay más de un sujeto en la imagen no formaban parte de este proyecto, pero es una buena mejora para desarrollar una mejor aplicación. OpenCV es un buen candidato para ayudar con este desarrollo, ya que es muy preciso para detectar heces y su posición en las imágenes, luego esa parte de la imagen (las caras) se puede clasificar por separado utilizando los modelos desarrollados en este proyecto."]},{"cell_type":"markdown","metadata":{"_uuid":"611ac23970abdadaae912cc3e9e3779265bd9fc1"},"source":["### 6. Let's play with the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5ad0cdabb6e82d73de3815a10cebf9f8df4b378b","trusted":true},"outputs":[],"source":["#dictionary to name the prediction\n","gender_target = {0: 'Female'\n","                , 1: 'Male'}\n","\n","def img_to_display(filename):\n","    # inspired on this kernel:\n","    # https://www.kaggle.com/stassl/displaying-inline-images-in-pandas-dataframe\n","    # credits to stassl :)\n","    \n","    i = Image.open(filename)\n","    i.thumbnail((200, 200), Image.LANCZOS)\n","    \n","    with BytesIO() as buffer:\n","        i.save(buffer, 'jpeg')\n","        return base64.b64encode(buffer.getvalue()).decode()\n","    \n","\n","def display_result(filename, prediction, target):\n","    '''\n","    Display the results in HTML\n","    \n","    '''\n","\n","    gender = 'Male'\n","    gender_icon = \"https://i.imgur.com/nxWan2u.png\"\n","        \n","    if prediction[1] <= 0.5:\n","        gender_icon = \"https://i.imgur.com/oAAb8rd.png\"\n","        gender = 'Female'\n","            \n","    display_html = '''\n","    <div style=\"overflow: auto;  border: 2px solid #D8D8D8;\n","        padding: 5px; width: 420px;\" >\n","        <img src=\"data:image/jpeg;base64,{}\" style=\"float: left;\" width=\"200\" height=\"200\">\n","        <div style=\"padding: 10px 0px 0px 20px; overflow: auto;\">\n","            <img src=\"{}\" style=\"float: left;\" width=\"40\" height=\"40\">\n","            <h3 style=\"margin-left: 50px; margin-top: 2px;\">{}</h3>\n","            <p style=\"margin-left: 50px; margin-top: -6px; font-size: 12px\">{} prob.</p>\n","            <p style=\"margin-left: 50px; margin-top: -16px; font-size: 12px\">Real Target: {}</p>\n","            <p style=\"margin-left: 50px; margin-top: -16px; font-size: 12px\">Filename: {}</p>\n","        </div>\n","    </div>\n","    '''.format(img_to_display(filename)\n","               , gender_icon\n","               , gender\n","               , \"{0:.2f}%\".format(round(max(prediction)*100,2))\n","               , gender_target[target]\n","               , filename.split('/')[-1]\n","               )\n","\n","    display(HTML(display_html))"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"19c28df51f9d3421ca2a89452ccc3f89509ff127","trusted":true},"outputs":[],"source":["def gender_prediction(filename):\n","    '''\n","    predict the gender\n","    \n","    input:\n","        filename: str of the file name\n","        \n","    return:\n","        array of the prob of the targets.\n","    \n","    '''\n","    \n","    im = cv2.imread(filename)\n","    im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (178, 218)).astype(np.float32) / 255.0\n","    im = np.expand_dims(im, axis =0)\n","    \n","    # prediction\n","    result = model_.predict(im)\n","    prediction = np.argmax(result)\n","    \n","    return result\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c272e8b54cccf4138f6ff806292ab4e5cda97db9","trusted":true},"outputs":[],"source":["#select random images of the test partition\n","df_to_test = df_par_attr[(df_par_attr['partition'] == 2)].sample(8)\n","\n","for index, target in df_to_test.iterrows():\n","    result = gender_prediction(images_folder + index)\n","    \n","    #display result\n","    display_result(images_folder + index, result[0], target['Male'])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"9a77ff0fc59a2c69a6c36cb7ff72e06c4fcbff8b4fa47948f6ef698602850972"}}},"nbformat":4,"nbformat_minor":4}
