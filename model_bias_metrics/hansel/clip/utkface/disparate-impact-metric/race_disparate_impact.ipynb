{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`device` will indicate wich `CLIP` model to use depending on the available hardware. If there is a GPU, `cuda:0` will be used  or `cuda:1` if there are multiple GPUs. If there is not a GPU, `cpu` will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['black', 'white', 'asian', 'indian']\n",
    "tkns = ['A photo of a person of color ' + label for label in labels]\n",
    "text = clip.tokenize(tkns).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tkns` is the domain of possible values. `CLIP` model predicts for each image the most probable sentence from `tkns`, in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100000\n",
    "\n",
    "dir_path = r'/Users/hanselblanco/Documents/4to/ML/UTKFace/UTKFace'\n",
    "ln = 0\n",
    "photo_paths = os.listdir(dir_path)\n",
    "\n",
    "for path in photo_paths:\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        ln += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing usefull variables for `CLIP` model application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [09:35<00:00, 575.29s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "photos_to_analize = 4000\n",
    "\n",
    "for i in tqdm(range(0, ln, BATCH_SIZE)):\n",
    "    images = [preprocess(Image.open(dir_path + '/' + photo_paths[j])) for j in range(photos_to_analize)]\n",
    "    image_input = torch.tensor(np.stack(images)).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        logits_per_image, logits_per_text = model(image_input, text)\n",
    "        # The softmax function takes the original confidence and applys a transform to make all the confidence add up to one\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "        results.append(probs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing `CLIP` model for `photos_to_analize` images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.concatenate(results,axis=0)\n",
    "choices = np.argmax(res,axis=1)\n",
    "choices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['asian', 'black', 'white', ..., 'white', 'black', 'asian'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getlabel = lambda x:labels[x]\n",
    "vgetlabel = np.vectorize(getlabel)\n",
    "colors = vgetlabel(choices)\n",
    "colors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`colors` is the vector with predicted labels to add to each sentence from `tkns` for each image, ordered."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting bias in `CLIP` results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Rate (Positive results / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9313380281690141, 0.8233082706766918, 0.9378427787934186, 0.8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_code = { 0 : 'white', 1 : 'black', 2 : 'asian', 3 : 'indian', 4 : 'others'}\n",
    "\n",
    "positive_whites, positive_blacks, positive_asians, positive_indians = 0, 0, 0, 0\n",
    "\n",
    "total_whites, total_blacks, total_asians, total_indians = 0, 0, 0, 0\n",
    "\n",
    "for i in range(photos_to_analize):\n",
    "    data = photo_paths[i].split('_')\n",
    "    race_number = int(data[2])\n",
    "    match race_code[race_number]:\n",
    "        case 'white':\n",
    "            total_whites += 1\n",
    "            if colors[i] == 'white':\n",
    "                positive_whites += 1\n",
    "        case 'black':\n",
    "            total_blacks += 1\n",
    "            if colors[i] == 'black':\n",
    "                positive_blacks += 1\n",
    "        case 'asian':\n",
    "            total_asians += 1\n",
    "            if colors[i] == 'asian':\n",
    "                positive_asians += 1\n",
    "        case 'indian':\n",
    "            total_indians += 1\n",
    "            if colors[i] == 'indian':\n",
    "                positive_indians += 1\n",
    "        case default:\n",
    "            continue\n",
    "                \n",
    "whites_sr, blacks_sr, asians_sr, indians_sr = positive_whites/ total_whites, positive_blacks/ total_blacks, positive_asians/ total_asians, positive_indians/ total_indians\n",
    "\n",
    "whites_sr, blacks_sr, asians_sr, indians_sr\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disparate impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8840058558494535, 0.8778745108384999, 1.0291353383458646)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# disparate impact ratio = underprivileged group SR / privileged group SR\n",
    "disp_impact_b_w = blacks_sr/ whites_sr\n",
    "disp_impact_b_a = asians_sr/ whites_sr\n",
    "disp_impact_b_i = indians_sr/ whites_sr\n",
    "disp_impact_b_w, disp_impact_b_a, disp_impact_b_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if disp_impact_b_w < 0.8:\n",
    "    print('Disparate impact present in black group / white group')\n",
    "if disp_impact_b_a < 0.8:\n",
    "    print('Disparate impact present in asian group / white group')\n",
    "if disp_impact_b_i < 0.8:\n",
    "    print('Disparate impact present in indian group / white group')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
