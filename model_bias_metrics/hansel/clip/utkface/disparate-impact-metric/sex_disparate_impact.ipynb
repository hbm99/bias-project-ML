{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['male', 'female']\n",
    "tkns = ['A photo of a person of sex ' + label for label in labels]\n",
    "text = clip.tokenize(tkns).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100000\n",
    "\n",
    "dir_path = r'/Users/hanselblanco/Documents/4to/ML/UTKFace/UTKFace'\n",
    "ln = 0\n",
    "photo_paths = os.listdir(dir_path)\n",
    "\n",
    "for path in photo_paths:\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        ln += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "photos_to_analize = 4000\n",
    "\n",
    "for i in tqdm(range(0, ln, BATCH_SIZE)):\n",
    "    images = [preprocess(Image.open(dir_path + '/' + photo_paths[j])) for j in range(photos_to_analize)]\n",
    "    image_input = torch.tensor(np.stack(images)).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        logits_per_image, logits_per_text = model(image_input, text)\n",
    "        # The softmax function takes the original confidence and applys a transform to make all the confidence add up to one\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "        results.append(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.concatenate(results,axis=0)\n",
    "choices = np.argmax(res,axis=1)\n",
    "choices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['female', 'male', 'female', ..., 'female', 'male', 'female'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getlabel = lambda x:labels[x]\n",
    "vgetlabel = np.vectorize(getlabel)\n",
    "genders = vgetlabel(choices)\n",
    "genders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Rate (Positive results / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9498580889309366, 0.9554612937433722)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_code = { 0 : 'male', 1 : 'female'}\n",
    "\n",
    "positive_males, positive_females = 0, 0\n",
    "\n",
    "total_males, total_females = 0, 0\n",
    "\n",
    "for i in range(photos_to_analize):\n",
    "    data = photo_paths[i].split('_')\n",
    "    gender_number = int(data[1])\n",
    "    match gender_code[gender_number]:\n",
    "        case 'male':\n",
    "            total_males += 1\n",
    "            if genders[i] == 'male':\n",
    "                positive_males += 1\n",
    "        case 'female':\n",
    "            total_females += 1\n",
    "            if genders[i] == 'female':\n",
    "                positive_females += 1\n",
    "                \n",
    "males_sr, females_sr = positive_males/ total_males, positive_females/ total_females\n",
    "\n",
    "males_sr, females_sr\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disparate impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9941356025104031"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# disparate impact ratio = underprivileged group SR / privileged group SR\n",
    "disp_impact = females_sr / males_sr\n",
    "disp_impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if disp_impact < 0.8:\n",
    "    print('Disparate impact present in female group / male group')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
